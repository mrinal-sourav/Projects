

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/100

9445953/9445953 [==============================] - 18s 2us/step - loss: 322.3900 - val_loss: 312.7393
Epoch 2/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 311.3080 - val_loss: 304.2145
Epoch 3/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 307.8732 - val_loss: 315.3043
Epoch 4/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 292.2568 - val_loss: 285.4798
Epoch 5/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 282.0184 - val_loss: 275.8171
Epoch 6/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 278.3869 - val_loss: 273.3062
Epoch 7/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 276.5456 - val_loss: 273.4373
Epoch 8/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 274.7531 - val_loss: 272.7294
Epoch 9/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 273.2769 - val_loss: 268.9000
Epoch 10/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 271.5840 - val_loss: 269.4318
Epoch 11/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 269.6144 - val_loss: 264.6621
Epoch 12/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 267.4231 - val_loss: 263.7067
Epoch 13/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 265.3515 - val_loss: 260.3060
Epoch 14/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 262.7073 - val_loss: 258.5520
Epoch 15/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 260.8006 - val_loss: 260.4388
Epoch 16/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 258.2583 - val_loss: 257.6066
Epoch 17/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 256.3881 - val_loss: 251.5301
Epoch 18/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 254.4252 - val_loss: 249.4695
Epoch 19/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 252.4439 - val_loss: 251.5976
Epoch 20/100
9445953/9445953 [==============================] - 17s 2us/step - loss: 250.8114 - val_loss: 254.3720
Epoch 21/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 249.1286 - val_loss: 249.7703
Epoch 22/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 247.6384 - val_loss: 242.4775
Epoch 23/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 246.0398 - val_loss: 241.9917
Epoch 24/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 244.5850 - val_loss: 239.1609
Epoch 25/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 243.4819 - val_loss: 244.3808
Epoch 26/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 242.0782 - val_loss: 250.6344
Epoch 27/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 240.7986 - val_loss: 241.1126
Epoch 28/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 239.7490 - val_loss: 238.7275
Epoch 29/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 238.4089 - val_loss: 237.4236
Epoch 30/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 237.5624 - val_loss: 238.9009
Epoch 31/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 236.5041 - val_loss: 239.3832
Epoch 32/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 235.3353 - val_loss: 235.6852
Epoch 33/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 234.5648 - val_loss: 235.3132
Epoch 34/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 233.6063 - val_loss: 235.5327
Epoch 35/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 233.0504 - val_loss: 231.3843
Epoch 36/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 231.7703 - val_loss: 232.0837
Epoch 37/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 231.0156 - val_loss: 234.2280
Epoch 38/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 230.3840 - val_loss: 232.1005
Epoch 39/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 229.5895 - val_loss: 232.9217
Epoch 40/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 228.6159 - val_loss: 233.3029
Epoch 41/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 228.1132 - val_loss: 230.7558
Epoch 42/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 227.5382 - val_loss: 225.2198
Epoch 43/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 226.5352 - val_loss: 224.5629
Epoch 44/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 226.2393 - val_loss: 227.2812
Epoch 45/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 225.5062 - val_loss: 226.3585
Epoch 46/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 224.6922 - val_loss: 229.9073
Epoch 47/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 224.1449 - val_loss: 224.2950
Epoch 48/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 223.7537 - val_loss: 222.5113
Epoch 49/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 223.2151 - val_loss: 225.2702
Epoch 50/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 222.6697 - val_loss: 223.5630
Epoch 51/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 222.0366 - val_loss: 222.8868
Epoch 52/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 221.7700 - val_loss: 221.0340
Epoch 53/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 221.1567 - val_loss: 221.8133
Epoch 54/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 220.7842 - val_loss: 227.1448
Epoch 55/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 220.2933 - val_loss: 222.8963
Epoch 56/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 219.7758 - val_loss: 222.7581
Epoch 57/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 219.3606 - val_loss: 227.2330
Epoch 58/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 219.0411 - val_loss: 227.6822
Epoch 59/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 218.6806 - val_loss: 221.3604
Epoch 60/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 218.0457 - val_loss: 219.4909
Epoch 61/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 217.7865 - val_loss: 223.8066
Epoch 62/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 217.6420 - val_loss: 225.5355
Epoch 63/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 217.1429 - val_loss: 224.0170
Epoch 64/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 216.6684 - val_loss: 221.8412
Epoch 65/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 216.2525 - val_loss: 217.8642
Epoch 66/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 216.1343 - val_loss: 216.9153
Epoch 67/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.9766 - val_loss: 226.3217
Epoch 68/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.4403 - val_loss: 221.8486
Epoch 69/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 215.2533 - val_loss: 223.6001
Epoch 70/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 214.6394 - val_loss: 218.1816
Epoch 71/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 214.4588 - val_loss: 219.3376
Epoch 72/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 214.2404 - val_loss: 215.2383
Epoch 73/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 214.0468 - val_loss: 228.0299
Epoch 74/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.6879 - val_loss: 220.1039
Epoch 75/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.4234 - val_loss: 219.5212
Epoch 76/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.2692 - val_loss: 225.6680
Epoch 77/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.1466 - val_loss: 216.9542
Epoch 78/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.4619 - val_loss: 221.1531
Epoch 79/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.6864 - val_loss: 221.0081
Epoch 80/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.2024 - val_loss: 225.6347
Epoch 81/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 211.8763 - val_loss: 218.2960
Epoch 82/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 211.7377 - val_loss: 213.6360
Epoch 83/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 211.3433 - val_loss: 212.6190
Epoch 84/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 211.3582 - val_loss: 220.4377
Epoch 85/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 211.1188 - val_loss: 217.2315
Epoch 86/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.7876 - val_loss: 218.7876
Epoch 87/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.4836 - val_loss: 219.9394
Epoch 88/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.6985 - val_loss: 223.0990
Epoch 89/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.2279 - val_loss: 217.3167
Epoch 90/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 210.1256 - val_loss: 215.7946
Epoch 91/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 209.8511 - val_loss: 220.4108
Epoch 92/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 209.8177 - val_loss: 216.5661
Epoch 93/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 209.5744 - val_loss: 212.6661
Epoch 94/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 209.2646 - val_loss: 219.4970
Epoch 95/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 209.3321 - val_loss: 216.9793
Epoch 96/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 208.9024 - val_loss: 216.8777
Epoch 97/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 208.6547 - val_loss: 213.7669
Epoch 98/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 208.6210 - val_loss: 217.2889
Epoch 99/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 208.5610 - val_loss: 211.8929
Epoch 100/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 208.4248 - val_loss: 216.3380

 ###### inititial_accuracy = 221.07795052861889



 QUEUE SIZE BEFORE GET = 1


 NODE SELECTED <<<------ Epochs = 100 -------- BatchSize = 30000 --------- Accuracy = 221.07795052861889 -------->>>


 CHILD 1 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 30000

         EPOCHS = 110
Train on 9445953 samples, validate on 1049551 samples
Epoch 1/110
2018-07-18 22:40:42.594081: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-18 22:40:42.600099: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 16s 2us/step - loss: 320.1973 - val_loss: 310.0303
Epoch 2/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 309.2479 - val_loss: 293.7634
Epoch 3/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 294.7448 - val_loss: 286.3327
Epoch 4/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 286.4721 - val_loss: 280.0473
Epoch 5/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 282.6847 - val_loss: 282.9840
Epoch 6/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 280.9814 - val_loss: 283.6415
Epoch 7/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 279.3882 - val_loss: 279.1715
Epoch 8/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 278.2966 - val_loss: 279.0916
Epoch 9/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 277.3058 - val_loss: 275.1813
Epoch 10/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 276.1070 - val_loss: 276.1783
Epoch 11/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 275.4826 - val_loss: 273.5238
Epoch 12/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 274.1149 - val_loss: 271.7211
Epoch 13/110
9445953/9445953 [==============================] - 19s 2us/step - loss: 273.0440 - val_loss: 269.3657
Epoch 14/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 271.5927 - val_loss: 268.0197
Epoch 15/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 269.9472 - val_loss: 268.5684
Epoch 16/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 267.8437 - val_loss: 263.4410
Epoch 17/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 264.7866 - val_loss: 259.5626
Epoch 18/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 261.1906 - val_loss: 266.4473
Epoch 19/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 258.0958 - val_loss: 250.9723
Epoch 20/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 255.2315 - val_loss: 256.2322
Epoch 21/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 253.0268 - val_loss: 249.5876
Epoch 22/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 250.8165 - val_loss: 245.4923
Epoch 23/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 248.9061 - val_loss: 248.6866
Epoch 24/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 247.0938 - val_loss: 248.9400
Epoch 25/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 245.6209 - val_loss: 245.8315
Epoch 26/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 244.1523 - val_loss: 242.1187
Epoch 27/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 242.5868 - val_loss: 246.1916
Epoch 28/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 241.2932 - val_loss: 239.5578
Epoch 29/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 240.2307 - val_loss: 247.2911
Epoch 30/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 238.9416 - val_loss: 236.0577
Epoch 31/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 237.6595 - val_loss: 237.3745
Epoch 32/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 236.7084 - val_loss: 234.7808
Epoch 33/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 236.0819 - val_loss: 236.8652
Epoch 34/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 234.9107 - val_loss: 235.6744
Epoch 35/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 234.0962 - val_loss: 230.6267
Epoch 36/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 233.2698 - val_loss: 229.8981
Epoch 37/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 232.4165 - val_loss: 234.0023
Epoch 38/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 231.9254 - val_loss: 232.0900
Epoch 39/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 230.9333 - val_loss: 236.9875
Epoch 40/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 230.3126 - val_loss: 231.1627
Epoch 41/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 229.5060 - val_loss: 231.6612
Epoch 42/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 228.9935 - val_loss: 232.9411
Epoch 43/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 228.2526 - val_loss: 231.4154
Epoch 44/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 227.7086 - val_loss: 229.8374
Epoch 45/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 227.0719 - val_loss: 226.1733
Epoch 46/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 226.3824 - val_loss: 228.0934
Epoch 47/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 226.0785 - val_loss: 226.2109
Epoch 48/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 225.2834 - val_loss: 224.6820
Epoch 49/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 224.8384 - val_loss: 226.2235
Epoch 50/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 224.4277 - val_loss: 228.4393
Epoch 51/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 224.0611 - val_loss: 231.1336
Epoch 52/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 223.5239 - val_loss: 224.9622
Epoch 53/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 222.8468 - val_loss: 224.8594
Epoch 54/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 222.4858 - val_loss: 223.6934
Epoch 55/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 222.2450 - val_loss: 226.3236
Epoch 56/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 221.6860 - val_loss: 223.6947
Epoch 57/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 221.1839 - val_loss: 227.6150
Epoch 58/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 220.8885 - val_loss: 222.2537
Epoch 59/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 220.7976 - val_loss: 225.5482
Epoch 60/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 220.3418 - val_loss: 227.0303
Epoch 61/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 219.6500 - val_loss: 225.4743
Epoch 62/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 219.7824 - val_loss: 223.7673
Epoch 63/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 219.2982 - val_loss: 226.9849
Epoch 64/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 218.9242 - val_loss: 231.5206
Epoch 65/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 218.5720 - val_loss: 234.1355
Epoch 66/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 218.3043 - val_loss: 225.8999
Epoch 67/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 217.9580 - val_loss: 228.8847
Epoch 68/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 217.8126 - val_loss: 221.5344
Epoch 69/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 217.1316 - val_loss: 218.8195
Epoch 70/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 217.3237 - val_loss: 217.4612
Epoch 71/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.8077 - val_loss: 230.9341
Epoch 72/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.6061 - val_loss: 221.3136
Epoch 73/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.4577 - val_loss: 231.7193
Epoch 74/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.0310 - val_loss: 221.0260
Epoch 75/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.0493 - val_loss: 223.5729
Epoch 76/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 215.4903 - val_loss: 221.9891
Epoch 77/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.4768 - val_loss: 228.9182
Epoch 78/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.0638 - val_loss: 221.2189
Epoch 79/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 214.9736 - val_loss: 219.7234
Epoch 80/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.0617 - val_loss: 224.1085
Epoch 81/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 214.4455 - val_loss: 227.9766
Epoch 82/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 214.4190 - val_loss: 226.8857
Epoch 83/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 214.1943 - val_loss: 222.4175
Epoch 84/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.9023 - val_loss: 221.8418
Epoch 85/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 213.8485 - val_loss: 219.1073
Epoch 86/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.7434 - val_loss: 221.1624
Epoch 87/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.1806 - val_loss: 223.1246
Epoch 88/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.2638 - val_loss: 220.4898
Epoch 89/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.8564 - val_loss: 217.2055
Epoch 90/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.9912 - val_loss: 224.0487
Epoch 91/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.9072 - val_loss: 222.9220
Epoch 92/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 212.4290 - val_loss: 221.5154
Epoch 93/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.4844 - val_loss: 225.2168
Epoch 94/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.3157 - val_loss: 217.7665
Epoch 95/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 211.8864 - val_loss: 222.2837
Epoch 96/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.0716 - val_loss: 218.3837
Epoch 97/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 211.7649 - val_loss: 222.9811
Epoch 98/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 211.6058 - val_loss: 218.5710
Epoch 99/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 211.6609 - val_loss: 219.5311
Epoch 100/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.4609 - val_loss: 219.0953
Epoch 101/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.9535 - val_loss: 221.2637
Epoch 102/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.3123 - val_loss: 219.4886
Epoch 103/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 210.8449 - val_loss: 222.7951
Epoch 104/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.5388 - val_loss: 217.6312
Epoch 105/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.7128 - val_loss: 222.8146
Epoch 106/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.4404 - val_loss: 214.0461
Epoch 107/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.3584 - val_loss: 218.1282
Epoch 108/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.1675 - val_loss: 218.6593
Epoch 109/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.2040 - val_loss: 219.9600
Epoch 110/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 210.0711 - val_loss: 226.3444

         CHILD ACCURACY = 231.34567077164584


         PARENT ACCURACY = 221.07795052861889


 CHILD 2 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 36000

         EPOCHS = 90
Train on 9445953 samples, validate on 1049551 samples
Epoch 1/90
2018-07-18 23:09:34.494316: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-18 23:09:34.503376: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 17s 2us/step - loss: 325.1427 - val_loss: 315.6973
Epoch 2/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 315.8760 - val_loss: 307.3536
Epoch 3/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 311.2942 - val_loss: 294.1904
Epoch 4/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 292.8717 - val_loss: 286.1206
Epoch 5/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 285.9868 - val_loss: 282.0144
Epoch 6/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 282.1926 - val_loss: 277.7078
Epoch 7/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 280.2115 - val_loss: 275.9327
Epoch 8/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 278.7722 - val_loss: 276.6524
Epoch 9/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 277.5053 - val_loss: 274.9161
Epoch 10/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 276.1053 - val_loss: 274.4664
Epoch 11/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 274.7507 - val_loss: 271.0076
Epoch 12/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 273.7834 - val_loss: 274.2495
Epoch 13/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 273.1721 - val_loss: 275.9584
Epoch 14/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 272.4374 - val_loss: 269.6844
Epoch 15/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 271.8453 - val_loss: 272.1102
Epoch 16/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 271.4559 - val_loss: 268.8616
Epoch 17/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 270.7283 - val_loss: 268.0257
Epoch 18/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 270.4605 - val_loss: 268.5746
Epoch 19/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 269.9759 - val_loss: 268.5192
Epoch 20/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 269.5992 - val_loss: 269.5569
Epoch 21/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 269.1856 - val_loss: 268.0321
Epoch 22/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 268.7937 - val_loss: 268.3797
Epoch 23/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 268.2669 - val_loss: 267.4962
Epoch 24/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 268.1065 - val_loss: 266.8329
Epoch 25/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 267.8118 - val_loss: 267.7924
Epoch 26/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 267.4733 - val_loss: 266.4256
Epoch 27/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 267.3609 - val_loss: 266.0640
Epoch 28/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 267.0994 - val_loss: 265.3760
Epoch 29/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 266.8720 - val_loss: 266.8123
Epoch 30/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 266.4414 - val_loss: 264.8673
Epoch 31/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 266.3794 - val_loss: 266.0281
Epoch 32/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 266.2200 - val_loss: 266.2251
Epoch 33/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 265.8470 - val_loss: 266.7367
Epoch 34/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 265.6778 - val_loss: 267.0062
Epoch 35/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 265.5423 - val_loss: 265.0523
Epoch 36/90
  36000/9445953 [..............................] - ETA: 1:59 - loss: 262.3242C

9445953/9445953 [==============================] - 16s 2us/step - loss: 265.4032 - val_loss: 265.4084
Epoch 37/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 265.1170 - val_loss: 265.2160
Epoch 38/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 264.9177 - val_loss: 264.9377
Epoch 39/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 264.8356 - val_loss: 264.9098
Epoch 40/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 264.6756 - val_loss: 264.3679
Epoch 41/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 264.4223 - val_loss: 264.8760
Epoch 42/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 264.1834 - val_loss: 264.5484
Epoch 43/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 264.1324 - val_loss: 265.4092
Epoch 44/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 264.0314 - val_loss: 264.3609
Epoch 45/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 263.6371 - val_loss: 267.4582
Epoch 46/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 263.5992 - val_loss: 263.7226
Epoch 47/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 263.4698 - val_loss: 264.3989
Epoch 48/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 263.3221 - val_loss: 263.5953
Epoch 49/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 262.9453 - val_loss: 264.7549
Epoch 50/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 262.8500 - val_loss: 265.8833
Epoch 51/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 262.6818 - val_loss: 263.6993
Epoch 52/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 262.4305 - val_loss: 264.4438
Epoch 53/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 262.3451 - val_loss: 263.6798
Epoch 54/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 262.0833 - val_loss: 264.8854
Epoch 55/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 261.7869 - val_loss: 263.5758
Epoch 56/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 261.6594 - val_loss: 264.6147
Epoch 57/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 261.3439 - val_loss: 262.4124
Epoch 58/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 261.0570 - val_loss: 263.4521
Epoch 59/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 260.8063 - val_loss: 263.2517
Epoch 60/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 260.4325 - val_loss: 261.6969
Epoch 61/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 260.0738 - val_loss: 261.9575
Epoch 62/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 259.6116 - val_loss: 262.5203
Epoch 63/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 258.9843 - val_loss: 258.7715
Epoch 64/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 257.6444 - val_loss: 257.8159
Epoch 65/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 254.5844 - val_loss: 248.7121
Epoch 66/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 251.3289 - val_loss: 249.2866
Epoch 67/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 248.4650 - val_loss: 247.5201
Epoch 68/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 245.9424 - val_loss: 246.1508
Epoch 69/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 243.6440 - val_loss: 242.1748
Epoch 70/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 241.9027 - val_loss: 241.9430
Epoch 71/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 240.0867 - val_loss: 241.6710
Epoch 72/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 238.7610 - val_loss: 242.8334
Epoch 73/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 237.1569 - val_loss: 235.8862
Epoch 74/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 235.9819 - val_loss: 236.6838
Epoch 75/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 234.7967 - val_loss: 236.1316
Epoch 76/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 233.8148 - val_loss: 232.8997
Epoch 77/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 232.5600 - val_loss: 235.7747
Epoch 78/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 232.1119 - val_loss: 232.3453
Epoch 79/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 231.1607 - val_loss: 229.3187
Epoch 80/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 230.3615 - val_loss: 228.9499
Epoch 81/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 229.7726 - val_loss: 230.2388
Epoch 82/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 228.9941 - val_loss: 233.5034
Epoch 83/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 228.2876 - val_loss: 228.0155
Epoch 84/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 227.7333 - val_loss: 229.7263
Epoch 85/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 227.3809 - val_loss: 226.9575
Epoch 86/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 226.6972 - val_loss: 225.7520
Epoch 87/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 226.4325 - val_loss: 227.7967
Epoch 88/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 225.6226 - val_loss: 239.5907
Epoch 89/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 225.3216 - val_loss: 234.8892
Epoch 90/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 224.9810 - val_loss: 234.8472

         CHILD ACCURACY = 242.97252227288982


         PARENT ACCURACY = 221.07795052861889


 CHILD 3 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 24000

         EPOCHS = 90
Train on 9445953 samples, validate on 1049551 samples
Epoch 1/90
2018-07-18 23:32:55.146589: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-18 23:32:55.153195: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 17s 2us/step - loss: 335.8074 - val_loss: 317.7574
Epoch 2/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 318.1720 - val_loss: 313.0402
Epoch 3/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 311.7711 - val_loss: 302.3101
Epoch 4/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 301.7985 - val_loss: 284.8523
Epoch 5/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 285.9143 - val_loss: 283.7379
Epoch 6/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 280.3685 - val_loss: 275.7821
Epoch 7/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 277.2581 - val_loss: 273.4585
Epoch 8/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 275.3774 - val_loss: 271.9049
Epoch 9/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 273.8546 - val_loss: 270.4187
Epoch 10/90
9445953/9445953 [==============================] - 17s 2us/step - loss: 272.5718 - val_loss: 272.4104
Epoch 11/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 271.1697 - val_loss: 266.4573
Epoch 12/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 269.4681 - val_loss: 266.5728
Epoch 13/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 267.8435 - val_loss: 262.9956
Epoch 14/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 265.7462 - val_loss: 259.3489
Epoch 15/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 263.4888 - val_loss: 257.1254
Epoch 16/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 260.9501 - val_loss: 259.7854
Epoch 17/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 258.3909 - val_loss: 256.1687
Epoch 18/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 256.0840 - val_loss: 256.3707
Epoch 19/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 253.8282 - val_loss: 249.3581
Epoch 20/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 251.6893 - val_loss: 252.1876
Epoch 21/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 249.9188 - val_loss: 248.8503
Epoch 22/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 248.0242 - val_loss: 242.7360
Epoch 23/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 246.4314 - val_loss: 250.0252
Epoch 24/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 244.5868 - val_loss: 248.5169
Epoch 25/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 243.0211 - val_loss: 239.7109
Epoch 26/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 241.6994 - val_loss: 249.8039
Epoch 27/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 240.3376 - val_loss: 245.1754
Epoch 28/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 239.0701 - val_loss: 241.2096
Epoch 29/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 237.8977 - val_loss: 240.0608
Epoch 30/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 236.6713 - val_loss: 244.5710
Epoch 31/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 235.7691 - val_loss: 237.1475
Epoch 32/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 234.7527 - val_loss: 232.0169
Epoch 33/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 233.6281 - val_loss: 235.5720
Epoch 34/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 232.6188 - val_loss: 231.8514
Epoch 35/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 231.5812 - val_loss: 232.4054
Epoch 36/90
9445953/9445953 [==============================] - 17s 2us/step - loss: 230.7862 - val_loss: 237.0096
Epoch 37/90
9445953/9445953 [==============================] - 17s 2us/step - loss: 230.0255 - val_loss: 229.6420
Epoch 38/90
9445953/9445953 [==============================] - 17s 2us/step - loss: 229.3182 - val_loss: 226.4412
Epoch 39/90
9445953/9445953 [==============================] - 17s 2us/step - loss: 228.3425 - val_loss: 227.8996
Epoch 40/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 227.4112 - val_loss: 225.2617
Epoch 41/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 227.0205 - val_loss: 227.3403
Epoch 42/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 226.2035 - val_loss: 227.6865
Epoch 43/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 225.9072 - val_loss: 235.5048
Epoch 44/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 224.8641 - val_loss: 222.1811
Epoch 45/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 224.4771 - val_loss: 225.6666
Epoch 46/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 223.7369 - val_loss: 229.8797
Epoch 47/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 223.1091 - val_loss: 226.9608
Epoch 48/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 222.5137 - val_loss: 228.9593
Epoch 49/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 222.1655 - val_loss: 225.7793
Epoch 50/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 221.5978 - val_loss: 221.9924
Epoch 51/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 220.9704 - val_loss: 228.9153
Epoch 52/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 220.4620 - val_loss: 226.3309
Epoch 53/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 219.9108 - val_loss: 218.7697
Epoch 54/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 219.6156 - val_loss: 231.8387
Epoch 55/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 219.3783 - val_loss: 220.4400
Epoch 56/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 218.5641 - val_loss: 217.7770
Epoch 57/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 218.5625 - val_loss: 219.1045
Epoch 58/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 217.9717 - val_loss: 217.3134
Epoch 59/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 217.6770 - val_loss: 216.3353
Epoch 60/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 217.1698 - val_loss: 226.0281
Epoch 61/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 216.7372 - val_loss: 223.3774
Epoch 62/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 216.4594 - val_loss: 221.2916
Epoch 63/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 216.1212 - val_loss: 216.3653
Epoch 64/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.7476 - val_loss: 220.3286
Epoch 65/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.3988 - val_loss: 220.4162
Epoch 66/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.0926 - val_loss: 220.3998
Epoch 67/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 214.5944 - val_loss: 220.9496
Epoch 68/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 214.4687 - val_loss: 217.9100
Epoch 69/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 214.1289 - val_loss: 219.4620
Epoch 70/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.6568 - val_loss: 226.4938
Epoch 71/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.5961 - val_loss: 228.4551
Epoch 72/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.2176 - val_loss: 212.9581
Epoch 73/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.8436 - val_loss: 225.6439
Epoch 74/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.7012 - val_loss: 216.0522
Epoch 75/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.5329 - val_loss: 213.5526
Epoch 76/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.1017 - val_loss: 212.6582
Epoch 77/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 211.9331 - val_loss: 219.3860
Epoch 78/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.8086 - val_loss: 214.0039
Epoch 79/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.1501 - val_loss: 219.0816
Epoch 80/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 211.0200 - val_loss: 219.1770
Epoch 81/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 211.0573 - val_loss: 213.0386
Epoch 82/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.5860 - val_loss: 211.1165
Epoch 83/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.3318 - val_loss: 215.9075
Epoch 84/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.5086 - val_loss: 220.3098
Epoch 85/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 210.1557 - val_loss: 211.5413
Epoch 86/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 209.8054 - val_loss: 226.3467
Epoch 87/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 209.7618 - val_loss: 214.1076
Epoch 88/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 209.5441 - val_loss: 215.1626
Epoch 89/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 209.2459 - val_loss: 223.9597
Epoch 90/90
9445953/9445953 [==============================] - 16s 2us/step - loss: 208.8151 - val_loss: 219.8892

         CHILD ACCURACY = 227.21776709860072


         PARENT ACCURACY = 221.07795052861889


 CHILD 4 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 24000

         EPOCHS = 110
Train on 9445953 samples, validate on 1049551 samples
Epoch 1/110
2018-07-18 23:56:51.483773: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-18 23:56:51.489714: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 16s 2us/step - loss: 325.2667 - val_loss: 314.2124
Epoch 2/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 313.6127 - val_loss: 309.1982
Epoch 3/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 316.2903 - val_loss: 305.7102
Epoch 4/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 299.4638 - val_loss: 285.7790
Epoch 5/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 288.4413 - val_loss: 283.0301
Epoch 6/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 284.4514 - val_loss: 282.6560
Epoch 7/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 281.1557 - val_loss: 280.7347
Epoch 8/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 278.8100 - val_loss: 277.1392
Epoch 9/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 276.8070 - val_loss: 273.6371
Epoch 10/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 275.7259 - val_loss: 273.2346
Epoch 11/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 274.3734 - val_loss: 272.6903
Epoch 12/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 273.6216 - val_loss: 276.7525
Epoch 13/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 272.8424 - val_loss: 270.3952
Epoch 14/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 272.1118 - val_loss: 270.0248
Epoch 15/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 271.4343 - val_loss: 269.9399
Epoch 16/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 270.8882 - val_loss: 271.1039
Epoch 17/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 270.3013 - val_loss: 269.0944
Epoch 18/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 269.7213 - val_loss: 268.0828
Epoch 19/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 269.2671 - val_loss: 267.1000
Epoch 20/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 268.7038 - val_loss: 269.2113
Epoch 21/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 268.1176 - val_loss: 267.6269
Epoch 22/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 267.2713 - val_loss: 264.8745
Epoch 23/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 265.7105 - val_loss: 268.2943
Epoch 24/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 261.8265 - val_loss: 255.7735
Epoch 25/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 257.9118 - val_loss: 255.5965
Epoch 26/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 254.5721 - val_loss: 249.2440
Epoch 27/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 251.5747 - val_loss: 254.0405
Epoch 28/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 248.8879 - val_loss: 247.1604
Epoch 29/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 246.4527 - val_loss: 245.2678
Epoch 30/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 244.3793 - val_loss: 243.2830
Epoch 31/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 242.5870 - val_loss: 241.5922
Epoch 32/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 240.8174 - val_loss: 239.5569
Epoch 33/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 239.3763 - val_loss: 235.6158
Epoch 34/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 237.6905 - val_loss: 239.8070
Epoch 35/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 236.8522 - val_loss: 235.8748
Epoch 36/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 235.4806 - val_loss: 233.6379
Epoch 37/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 234.4911 - val_loss: 235.5208
Epoch 38/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 233.4342 - val_loss: 231.7224
Epoch 39/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 232.6564 - val_loss: 232.1976
Epoch 40/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 231.7544 - val_loss: 239.5056
Epoch 41/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 230.9480 - val_loss: 230.4475
Epoch 42/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 230.2110 - val_loss: 233.1312
Epoch 43/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 229.6208 - val_loss: 227.7445
Epoch 44/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 228.8614 - val_loss: 228.9864
Epoch 45/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 228.3185 - val_loss: 234.2237
Epoch 46/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 227.6609 - val_loss: 228.9832
Epoch 47/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 227.2844 - val_loss: 225.9934
Epoch 48/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 226.7117 - val_loss: 228.9437
Epoch 49/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 225.8405 - val_loss: 240.0627
Epoch 50/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 225.5921 - val_loss: 224.0689
Epoch 51/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 225.0543 - val_loss: 224.7303
Epoch 52/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 224.5672 - val_loss: 223.6057
Epoch 53/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 224.2650 - val_loss: 222.8333
Epoch 54/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 223.6309 - val_loss: 226.0740
Epoch 55/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 223.4145 - val_loss: 223.1004
Epoch 56/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 223.0657 - val_loss: 223.5442
Epoch 57/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 222.4786 - val_loss: 223.6672
Epoch 58/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 222.0887 - val_loss: 224.4219
Epoch 59/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 221.7627 - val_loss: 228.5093
Epoch 60/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 221.5882 - val_loss: 229.5465
Epoch 61/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 221.2616 - val_loss: 228.2839
Epoch 62/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 220.9954 - val_loss: 226.6593
Epoch 63/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 220.4211 - val_loss: 224.1362
Epoch 64/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 220.3022 - val_loss: 224.8761
Epoch 65/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 219.8834 - val_loss: 218.5323
Epoch 66/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 219.6692 - val_loss: 220.0147
Epoch 67/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 219.2995 - val_loss: 222.5041
Epoch 68/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 219.1805 - val_loss: 225.0870
Epoch 69/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 219.0182 - val_loss: 222.3921
Epoch 70/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 218.7555 - val_loss: 218.7042
Epoch 71/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 218.3451 - val_loss: 222.2456
Epoch 72/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 218.0043 - val_loss: 215.8336
Epoch 73/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 217.9609 - val_loss: 222.5352
Epoch 74/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 217.7600 - val_loss: 219.7092
Epoch 75/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 217.4808 - val_loss: 221.3898
Epoch 76/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 217.2583 - val_loss: 221.7314
Epoch 77/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 216.9976 - val_loss: 223.2716
Epoch 78/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 216.7783 - val_loss: 225.0915
Epoch 79/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 216.4936 - val_loss: 216.6500
Epoch 80/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 216.5307 - val_loss: 219.0448
Epoch 81/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 216.1390 - val_loss: 215.6397
Epoch 82/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.9222 - val_loss: 224.0956
Epoch 83/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.9254 - val_loss: 217.4003
Epoch 84/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.6347 - val_loss: 226.7525
Epoch 85/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.4245 - val_loss: 219.5274
Epoch 86/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.3221 - val_loss: 218.7972
Epoch 87/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.2547 - val_loss: 232.1306
Epoch 88/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 215.0454 - val_loss: 220.2581
Epoch 89/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 214.6311 - val_loss: 217.1630
Epoch 90/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 214.8563 - val_loss: 224.9846
Epoch 91/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 214.4179 - val_loss: 232.8500
Epoch 92/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 214.2328 - val_loss: 224.7681
Epoch 93/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 214.2433 - val_loss: 222.1730
Epoch 94/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.9761 - val_loss: 218.3010
Epoch 95/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 213.7549 - val_loss: 217.2051
Epoch 96/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.9015 - val_loss: 218.3697
Epoch 97/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.4390 - val_loss: 223.8459
Epoch 98/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.5893 - val_loss: 218.4067
Epoch 99/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.1674 - val_loss: 214.8632
Epoch 100/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.0269 - val_loss: 214.8096
Epoch 101/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 213.1916 - val_loss: 227.1132
Epoch 102/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.9984 - val_loss: 218.2398
Epoch 103/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.8255 - val_loss: 217.0445
Epoch 104/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.4983 - val_loss: 219.3454
Epoch 105/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 212.5816 - val_loss: 226.4383
Epoch 106/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.3354 - val_loss: 216.0538
Epoch 107/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.3352 - val_loss: 216.3319
Epoch 108/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.0907 - val_loss: 224.7010
Epoch 109/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.1519 - val_loss: 217.7914
Epoch 110/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 212.0062 - val_loss: 220.2056

         CHILD ACCURACY = 227.368951006004


         PARENT ACCURACY = 221.07795052861889


 CHILD 5 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 36000

         EPOCHS = 110
Train on 9445953 samples, validate on 1049551 samples
Epoch 1/110
2018-07-19 00:26:15.799362: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-19 00:26:15.808405: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 16s 2us/step - loss: 323.2404 - val_loss: 316.0678
Epoch 2/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 317.3939 - val_loss: 313.0704
Epoch 3/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 314.3119 - val_loss: 306.3201
Epoch 4/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 311.0954 - val_loss: 289.9185
Epoch 5/110
9445953/9445953 [==============================] - 17s 2us/step - loss: 293.9190 - val_loss: 285.5389
Epoch 6/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 286.6410 - val_loss: 280.9987
Epoch 7/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 283.2084 - val_loss: 279.6598
Epoch 8/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 280.6232 - val_loss: 278.8790
Epoch 9/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 279.2481 - val_loss: 278.4454
Epoch 10/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 278.1753 - val_loss: 275.1850
Epoch 11/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 276.9731 - val_loss: 275.2352
Epoch 12/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 275.9500 - val_loss: 272.7343
Epoch 13/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 274.6412 - val_loss: 272.1170
Epoch 14/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 273.6439 - val_loss: 275.2218
Epoch 15/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 272.0571 - val_loss: 270.5757
Epoch 16/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 270.8576 - val_loss: 265.4161
Epoch 17/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 269.1767 - val_loss: 267.9544
Epoch 18/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 267.6331 - val_loss: 274.2462
Epoch 19/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 265.7776 - val_loss: 264.9429
Epoch 20/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 264.1667 - val_loss: 263.0298
Epoch 21/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 262.4308 - val_loss: 256.6852
Epoch 22/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 260.9122 - val_loss: 264.4687
Epoch 23/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 258.9397 - val_loss: 260.5607
Epoch 24/110
9445953/9445953 [==============================] - 16s 2us/step - loss: 257.2214 - val_loss: 254.0582
Epoch 25/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 254.6424 - val_loss: 253.3963
Epoch 26/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 253.1652 - val_loss: 250.0371
Epoch 27/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 250.7952 - val_loss: 252.6073
Epoch 28/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 249.3966 - val_loss: 243.6663
Epoch 29/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 247.6386 - val_loss: 253.0533
Epoch 30/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 246.3837 - val_loss: 245.3168
Epoch 31/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 244.7907 - val_loss: 246.6202
Epoch 32/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 243.5450 - val_loss: 252.6505
Epoch 33/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 242.4596 - val_loss: 243.7471
Epoch 34/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 241.2877 - val_loss: 242.1207
Epoch 35/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 240.1561 - val_loss: 239.5592
Epoch 36/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 239.1349 - val_loss: 238.7874
Epoch 37/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 238.1036 - val_loss: 240.1181
Epoch 38/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 237.0015 - val_loss: 246.4371
Epoch 39/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 236.0933 - val_loss: 242.1174
Epoch 40/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 235.4356 - val_loss: 239.7902
Epoch 41/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 234.3506 - val_loss: 230.0335
Epoch 42/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 233.5158 - val_loss: 231.2817
Epoch 43/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 232.6213 - val_loss: 231.8747
Epoch 44/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 231.9943 - val_loss: 237.4068
Epoch 45/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 231.1116 - val_loss: 232.9353
Epoch 46/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 230.4500 - val_loss: 233.1764
Epoch 47/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 229.6480 - val_loss: 230.1826
Epoch 48/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 229.1021 - val_loss: 230.3680
Epoch 49/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 228.4224 - val_loss: 231.1984
Epoch 50/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 228.1031 - val_loss: 233.5406
Epoch 51/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 227.1817 - val_loss: 232.9967
Epoch 52/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 226.5452 - val_loss: 227.9260
Epoch 53/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 226.0371 - val_loss: 234.0793
Epoch 54/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 225.3379 - val_loss: 225.4065
Epoch 55/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 225.0252 - val_loss: 234.8509
Epoch 56/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 224.5734 - val_loss: 225.8506
Epoch 57/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 223.7825 - val_loss: 234.4587
Epoch 58/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 223.2566 - val_loss: 227.2379
Epoch 59/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 222.7074 - val_loss: 224.1896
Epoch 60/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 222.5621 - val_loss: 225.9829
Epoch 61/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 222.2311 - val_loss: 225.8972
Epoch 62/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 221.6209 - val_loss: 227.0203
Epoch 63/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 221.0431 - val_loss: 228.9475
Epoch 64/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 220.5630 - val_loss: 221.7370
Epoch 65/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 220.3562 - val_loss: 223.4645
Epoch 66/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 219.5696 - val_loss: 219.5815
Epoch 67/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 219.5549 - val_loss: 226.3665
Epoch 68/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 219.2820 - val_loss: 223.8698
Epoch 69/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 218.8617 - val_loss: 224.6987
Epoch 70/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 218.2690 - val_loss: 225.0211
Epoch 71/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 218.1746 - val_loss: 228.9810
Epoch 72/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 217.8996 - val_loss: 222.8629
Epoch 73/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 217.2800 - val_loss: 224.5926
Epoch 74/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.9359 - val_loss: 222.6011
Epoch 75/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.5720 - val_loss: 225.7243
Epoch 76/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.3727 - val_loss: 217.8882
Epoch 77/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 215.9605 - val_loss: 225.2616
Epoch 78/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.0231 - val_loss: 220.8233
Epoch 79/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 215.2457 - val_loss: 219.0324
Epoch 80/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 215.3641 - val_loss: 216.9172
Epoch 81/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 214.7043 - val_loss: 232.5777
Epoch 82/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 214.5177 - val_loss: 227.5474
Epoch 83/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 214.2380 - val_loss: 218.1449
Epoch 84/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 214.3240 - val_loss: 224.8298
Epoch 85/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 213.5858 - val_loss: 219.5141
Epoch 86/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 213.8059 - val_loss: 218.4275
Epoch 87/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 213.2793 - val_loss: 217.1999
Epoch 88/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 213.1018 - val_loss: 222.6979
Epoch 89/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 212.8261 - val_loss: 217.0389
Epoch 90/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 212.6696 - val_loss: 219.7515
Epoch 91/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 212.1736 - val_loss: 214.1180
Epoch 92/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 212.4220 - val_loss: 228.5434
Epoch 93/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.7948 - val_loss: 222.7330
Epoch 94/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.7845 - val_loss: 216.7621
Epoch 95/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.3866 - val_loss: 232.0668
Epoch 96/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.1714 - val_loss: 218.2549
Epoch 97/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.0193 - val_loss: 226.3915
Epoch 98/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.0642 - val_loss: 221.6883
Epoch 99/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 210.4778 - val_loss: 226.2103
Epoch 100/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 210.4275 - val_loss: 217.3803
Epoch 101/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 210.3451 - val_loss: 222.6143
Epoch 102/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 210.2759 - val_loss: 215.6606
Epoch 103/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 209.6970 - val_loss: 215.1308
Epoch 104/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 209.7949 - val_loss: 218.2711
Epoch 105/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 209.3002 - val_loss: 225.2381
Epoch 106/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 209.4736 - val_loss: 221.3772
Epoch 107/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 209.1252 - val_loss: 214.0595
Epoch 108/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 209.0308 - val_loss: 217.3626
Epoch 109/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 208.6216 - val_loss: 218.2399
Epoch 110/110
9445953/9445953 [==============================] - 15s 2us/step - loss: 208.6427 - val_loss: 224.2773

         CHILD ACCURACY = 228.11465243184344


         PARENT ACCURACY = 221.07795052861889


 CHILD 6 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 30000

         EPOCHS = 90
Train on 9445953 samples, validate on 1049551 samples
Epoch 1/90
2018-07-19 00:54:22.050851: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-19 00:54:22.101854: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 16s 2us/step - loss: 319.6092 - val_loss: 312.1850
Epoch 2/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 306.5291 - val_loss: 295.9551
Epoch 3/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 290.1719 - val_loss: 285.1007
Epoch 4/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 284.4750 - val_loss: 278.1230
Epoch 5/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 280.7395 - val_loss: 274.2176
Epoch 6/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 278.3127 - val_loss: 275.4608
Epoch 7/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 276.4651 - val_loss: 271.8930
Epoch 8/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 275.1051 - val_loss: 276.5930
Epoch 9/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 274.2433 - val_loss: 271.6163
Epoch 10/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 273.2567 - val_loss: 274.7889
Epoch 11/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 272.5588 - val_loss: 269.0789
Epoch 12/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 272.1974 - val_loss: 272.2565
Epoch 13/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 271.4020 - val_loss: 268.1182
Epoch 14/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 271.0101 - val_loss: 267.9293
Epoch 15/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 270.5008 - val_loss: 267.5550
Epoch 16/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 269.9156 - val_loss: 267.1163
Epoch 17/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 269.5305 - val_loss: 267.5235
Epoch 18/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 269.1576 - val_loss: 268.9884
Epoch 19/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 268.7566 - val_loss: 267.1789
Epoch 20/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 268.4487 - val_loss: 266.5959
Epoch 21/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 267.9156 - val_loss: 266.0283
Epoch 22/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 267.6123 - val_loss: 266.2792
Epoch 23/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 267.1873 - val_loss: 265.5099
Epoch 24/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 266.9835 - val_loss: 265.2438
Epoch 25/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 266.4966 - val_loss: 265.0982
Epoch 26/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 266.2363 - val_loss: 264.9526
Epoch 27/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 265.9188 - val_loss: 264.2886
Epoch 28/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 265.5847 - val_loss: 264.0635
Epoch 29/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 265.3372 - val_loss: 263.7729
Epoch 30/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 264.9757 - val_loss: 264.4381
Epoch 31/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 264.6344 - val_loss: 263.4704
Epoch 32/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 264.4869 - val_loss: 264.2151
Epoch 33/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 263.9947 - val_loss: 263.2476
Epoch 34/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 263.7756 - val_loss: 263.4365
Epoch 35/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 263.4085 - val_loss: 262.4226
Epoch 36/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 263.0017 - val_loss: 261.9483
Epoch 37/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 262.4975 - val_loss: 261.7067
Epoch 38/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 261.5873 - val_loss: 259.4167
Epoch 39/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 259.5265 - val_loss: 257.2388
Epoch 40/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 256.0210 - val_loss: 255.6039
Epoch 41/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 252.4265 - val_loss: 253.1621
Epoch 42/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 249.3587 - val_loss: 247.4471
Epoch 43/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 246.7761 - val_loss: 242.5365
Epoch 44/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 244.3845 - val_loss: 245.4382
Epoch 45/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 242.1253 - val_loss: 239.9375
Epoch 46/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 240.5633 - val_loss: 237.5373
Epoch 47/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 238.6075 - val_loss: 238.9060
Epoch 48/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 237.5159 - val_loss: 235.5985
Epoch 49/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 235.9944 - val_loss: 242.2599
Epoch 50/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 234.8280 - val_loss: 235.5961
Epoch 51/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 233.4986 - val_loss: 232.2552
Epoch 52/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 232.6708 - val_loss: 228.0958
Epoch 53/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 231.5638 - val_loss: 228.9030
Epoch 54/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 230.6158 - val_loss: 233.5104
Epoch 55/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 229.6374 - val_loss: 235.4260
Epoch 56/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 229.2608 - val_loss: 230.5158
Epoch 57/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 227.9454 - val_loss: 231.4221
Epoch 58/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 227.4211 - val_loss: 228.5970
Epoch 59/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 227.1249 - val_loss: 225.8406
Epoch 60/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 226.3235 - val_loss: 230.9493
Epoch 61/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 225.8350 - val_loss: 229.5263
Epoch 62/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 224.8201 - val_loss: 226.3863
Epoch 63/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 224.6062 - val_loss: 222.5608
Epoch 64/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 224.0317 - val_loss: 230.8464
Epoch 65/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 223.4226 - val_loss: 236.0919
Epoch 66/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 223.2270 - val_loss: 223.6643
Epoch 67/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 222.5706 - val_loss: 230.5841
Epoch 68/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 222.0555 - val_loss: 223.1981
Epoch 69/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 221.7683 - val_loss: 227.8370
Epoch 70/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 221.3641 - val_loss: 221.8482
Epoch 71/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 220.8092 - val_loss: 226.8675
Epoch 72/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 220.6453 - val_loss: 222.0988
Epoch 73/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 220.2348 - val_loss: 222.9841
Epoch 74/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 219.8296 - val_loss: 223.8663
Epoch 75/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 219.4788 - val_loss: 221.2758
Epoch 76/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 219.2752 - val_loss: 223.0632
Epoch 77/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 218.9180 - val_loss: 219.2225
Epoch 78/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 218.6485 - val_loss: 221.2761
Epoch 79/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 218.2968 - val_loss: 226.5146
Epoch 80/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 217.9181 - val_loss: 222.6293
Epoch 81/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 217.7990 - val_loss: 222.6989
Epoch 82/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 217.5927 - val_loss: 226.8906
Epoch 83/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 217.1998 - val_loss: 217.3126
Epoch 84/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.8653 - val_loss: 223.5954
Epoch 85/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.8988 - val_loss: 228.0225
Epoch 86/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.4059 - val_loss: 223.4735
Epoch 87/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.2940 - val_loss: 220.8103
Epoch 88/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 215.9315 - val_loss: 222.0106
Epoch 89/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 215.5564 - val_loss: 228.4677
Epoch 90/90
9445953/9445953 [==============================] - 15s 2us/step - loss: 215.5066 - val_loss: 219.5854

         CHILD ACCURACY = 229.6148496937252


         PARENT ACCURACY = 221.07795052861889


 CHILD 7 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 24000

         EPOCHS = 100
Train on 9445953 samples, validate on 1049551 samples
Epoch 1/100
2018-07-19 01:17:15.125711: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-19 01:17:15.129807: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 16s 2us/step - loss: 368.7423 - val_loss: 318.4791
Epoch 2/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 321.0115 - val_loss: 318.7980
Epoch 3/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 317.9524 - val_loss: 312.7951
Epoch 4/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 313.4512 - val_loss: 300.5816
Epoch 5/100
9445953/9445953 [==============================] - 16s 2us/step - loss: 358.0414 - val_loss: 363.7451
Epoch 6/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 359.4487 - val_loss: 350.9837
Epoch 7/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 348.9069 - val_loss: 342.2257
Epoch 8/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 341.2636 - val_loss: 335.6266
Epoch 9/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 335.4056 - val_loss: 330.5157
Epoch 10/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 330.8539 - val_loss: 326.5510
Epoch 11/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 327.3362 - val_loss: 323.5148
Epoch 12/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 324.6674 - val_loss: 321.2494
Epoch 13/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 322.7046 - val_loss: 319.6235
Epoch 14/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 321.3235 - val_loss: 318.5160
Epoch 15/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 320.4076 - val_loss: 317.8150
Epoch 16/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.8469 - val_loss: 317.4124
Epoch 17/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.5375 - val_loss: 317.2095
Epoch 18/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3886 - val_loss: 317.1237
Epoch 19/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3282 - val_loss: 317.0955
Epoch 20/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3087 - val_loss: 317.0897
Epoch 21/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3039 - val_loss: 317.0894
Epoch 22/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0898
Epoch 23/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0901
Epoch 24/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0899
Epoch 25/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0899
Epoch 26/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0897
Epoch 27/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0902
Epoch 28/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0907
Epoch 29/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0902
Epoch 30/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0900
Epoch 31/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0897
Epoch 32/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0900
Epoch 33/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0896
Epoch 34/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0897
Epoch 35/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0898
Epoch 36/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0910
Epoch 37/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 38/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0894
Epoch 39/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0901
Epoch 40/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0896
Epoch 41/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0904
Epoch 42/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0896
Epoch 43/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0900
Epoch 44/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0898
Epoch 45/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0898
Epoch 46/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0903
Epoch 47/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0906
Epoch 48/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0894
Epoch 49/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0897
Epoch 50/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0905
Epoch 51/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0899
Epoch 52/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0897
Epoch 53/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0910
Epoch 54/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0897
Epoch 55/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0897
Epoch 56/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0899
Epoch 57/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0903
Epoch 58/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0906
Epoch 59/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0898
Epoch 60/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0901
Epoch 61/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0894
Epoch 62/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0903
Epoch 63/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0907
Epoch 64/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0899
Epoch 65/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0895
Epoch 66/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0903
Epoch 67/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0897
Epoch 68/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0902
Epoch 69/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3032 - val_loss: 317.0901
Epoch 70/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0897
Epoch 71/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0905
Epoch 72/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0903
Epoch 73/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0911
Epoch 74/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0909
Epoch 75/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0896
Epoch 76/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0907
Epoch 77/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0912
Epoch 78/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 79/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0902
Epoch 80/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0904
Epoch 81/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0905
Epoch 82/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0894
Epoch 83/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0911
Epoch 84/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0899
Epoch 85/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0899
Epoch 86/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0902
Epoch 87/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0901
Epoch 88/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0901
Epoch 89/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0908
Epoch 90/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0910
Epoch 91/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0898
Epoch 92/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0898
Epoch 93/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0896
Epoch 94/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0904
Epoch 95/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0898
Epoch 96/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3031 - val_loss: 317.0896
Epoch 97/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0896
Epoch 98/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0911
Epoch 99/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 100/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 319.3030 - val_loss: 317.0902

         CHILD ACCURACY = 314.87736091769926


         PARENT ACCURACY = 221.07795052861889


 CHILD 8 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 36000

         EPOCHS = 100
Train on 9445953 samples, validate on 1049551 samples
Epoch 1/100
2018-07-19 01:42:37.855466: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-19 01:42:37.861383: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 16s 2us/step - loss: 320.9935 - val_loss: 311.5055
Epoch 2/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 312.2456 - val_loss: 303.6539
Epoch 3/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 299.4029 - val_loss: 296.6339
Epoch 4/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 291.2996 - val_loss: 286.4166
Epoch 5/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 286.5180 - val_loss: 283.0249
Epoch 6/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 283.0379 - val_loss: 278.9174
Epoch 7/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 280.3982 - val_loss: 276.7734
Epoch 8/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 278.6405 - val_loss: 273.9379
Epoch 9/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 276.9708 - val_loss: 274.5114
Epoch 10/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 275.4478 - val_loss: 277.6889
Epoch 11/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 274.3822 - val_loss: 271.4080
Epoch 12/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 272.1683 - val_loss: 277.9662
Epoch 13/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 270.0829 - val_loss: 263.1407
Epoch 14/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 267.2234 - val_loss: 268.2340
Epoch 15/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 264.8353 - val_loss: 261.9251
Epoch 16/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 262.6403 - val_loss: 257.7751
Epoch 17/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 260.6716 - val_loss: 259.5115
Epoch 18/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 258.8740 - val_loss: 254.6235
Epoch 19/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 256.8666 - val_loss: 253.1296
Epoch 20/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 255.0935 - val_loss: 255.7936
Epoch 21/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 253.3600 - val_loss: 252.8589
Epoch 22/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 251.8688 - val_loss: 254.9538
Epoch 23/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 250.1409 - val_loss: 246.2384
Epoch 24/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 248.7978 - val_loss: 244.0659
Epoch 25/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 247.4259 - val_loss: 245.2919
Epoch 26/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 246.1464 - val_loss: 248.0099
Epoch 27/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 245.2643 - val_loss: 251.2640
Epoch 28/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 243.6736 - val_loss: 246.0755
Epoch 29/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 242.5259 - val_loss: 246.2137
Epoch 30/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 241.4311 - val_loss: 244.3147
Epoch 31/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 240.5890 - val_loss: 237.9373
Epoch 32/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 239.4090 - val_loss: 245.9540
Epoch 33/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 238.6152 - val_loss: 240.5427
Epoch 34/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 237.2584 - val_loss: 236.3737
Epoch 35/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 236.4208 - val_loss: 242.3870
Epoch 36/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 235.8048 - val_loss: 238.3731
Epoch 37/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 234.7614 - val_loss: 233.0151
Epoch 38/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 233.9130 - val_loss: 236.5653
Epoch 39/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 233.0908 - val_loss: 233.3777
Epoch 40/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 232.4910 - val_loss: 232.5892
Epoch 41/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 231.5481 - val_loss: 232.9126
Epoch 42/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 231.0601 - val_loss: 233.7220
Epoch 43/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 230.1711 - val_loss: 230.1636
Epoch 44/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 229.5321 - val_loss: 240.6551
Epoch 45/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 228.9034 - val_loss: 232.6398
Epoch 46/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 228.2216 - val_loss: 231.8221
Epoch 47/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 227.5658 - val_loss: 233.7971
Epoch 48/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 227.2278 - val_loss: 235.8113
Epoch 49/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 226.4372 - val_loss: 230.6554
Epoch 50/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 225.8342 - val_loss: 228.3633
Epoch 51/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 225.3328 - val_loss: 232.5489
Epoch 52/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 224.8196 - val_loss: 228.8717
Epoch 53/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 224.2194 - val_loss: 230.7398
Epoch 54/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 223.4760 - val_loss: 224.9905
Epoch 55/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 223.0025 - val_loss: 227.2318
Epoch 56/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 222.7018 - val_loss: 236.3057
Epoch 57/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 222.4032 - val_loss: 226.1768
Epoch 58/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 221.3707 - val_loss: 233.9701
Epoch 59/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 221.4050 - val_loss: 238.5481
Epoch 60/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 220.9029 - val_loss: 224.7456
Epoch 61/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 220.2849 - val_loss: 235.6974
Epoch 62/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 220.3863 - val_loss: 228.3112
Epoch 63/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 219.4050 - val_loss: 224.3359
Epoch 64/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 219.2975 - val_loss: 222.7569
Epoch 65/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 218.8029 - val_loss: 225.2048
Epoch 66/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 218.2047 - val_loss: 229.4874
Epoch 67/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 218.0352 - val_loss: 225.0732
Epoch 68/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 217.7610 - val_loss: 223.8186
Epoch 69/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 217.4014 - val_loss: 225.3727
Epoch 70/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 217.0243 - val_loss: 228.5646
Epoch 71/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.6781 - val_loss: 229.3490
Epoch 72/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.5023 - val_loss: 222.8265
Epoch 73/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 216.1788 - val_loss: 226.0267
Epoch 74/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 215.8125 - val_loss: 224.3150
Epoch 75/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 215.5007 - val_loss: 221.2703
Epoch 76/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 215.1830 - val_loss: 222.6793
Epoch 77/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 214.8155 - val_loss: 230.4174
Epoch 78/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 214.7133 - val_loss: 221.2008
Epoch 79/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 214.1922 - val_loss: 220.6003
Epoch 80/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 214.0727 - val_loss: 218.0648
Epoch 81/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 213.6410 - val_loss: 225.1854
Epoch 82/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 213.3348 - val_loss: 224.0384
Epoch 83/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 213.2990 - val_loss: 223.4476
Epoch 84/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 212.8640 - val_loss: 216.8442
Epoch 85/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 212.9039 - val_loss: 217.6871
Epoch 86/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 212.5029 - val_loss: 227.5580
Epoch 87/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 212.2341 - val_loss: 218.7608
Epoch 88/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 212.0748 - val_loss: 224.9426
Epoch 89/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 212.0581 - val_loss: 231.5403
Epoch 90/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.5561 - val_loss: 220.6930
Epoch 91/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.1801 - val_loss: 225.3843
Epoch 92/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.2438 - val_loss: 227.9408
Epoch 93/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 211.1106 - val_loss: 218.8012
Epoch 94/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 210.8519 - val_loss: 217.2241
Epoch 95/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 210.6755 - val_loss: 227.4040
Epoch 96/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 210.4545 - val_loss: 216.0240
Epoch 97/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 210.0450 - val_loss: 226.1675
Epoch 98/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 210.0632 - val_loss: 219.2439
Epoch 99/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 209.7755 - val_loss: 220.9264
Epoch 100/100
9445953/9445953 [==============================] - 15s 2us/step - loss: 209.6390 - val_loss: 220.8066

         CHILD ACCURACY = 233.69146676604706


         PARENT ACCURACY = 221.07795052861889


 NODES EXPLORED:
(110, 36000)
(90, 24000)
(110, 24000)
(90, 36000)
(100, 36000)
(100, 24000)
(110, 30000)
(90, 30000)



 #########################  TRAINING COMPLETED  ###############################


