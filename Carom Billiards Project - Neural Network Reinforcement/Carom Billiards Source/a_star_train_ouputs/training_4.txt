
C:\Users\mrinal\Documents\code_stuff\Carom Billiards Source>python nn_a_star_train.py
C:\Users\mrinal\AppData\Local\Programs\Python\Python36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Train on 9445953 samples, validate on 1049551 samples
Epoch 1/43
2018-07-16 14:04:47.918741: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-07-16 14:04:48.927842: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1212] Found device 0 with properties:
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.30GiB
2018-07-16 14:04:48.935272: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 14:04:49.600772: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3032 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 23s 2us/step - loss: 301.8385 - val_loss: 282.5166
Epoch 2/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 283.4779 - val_loss: 281.1651
Epoch 3/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 277.8765 - val_loss: 272.4081
Epoch 4/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 272.3842 - val_loss: 266.1987
Epoch 5/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 265.7124 - val_loss: 256.3612
Epoch 6/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 258.2471 - val_loss: 252.1994
Epoch 7/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 252.6483 - val_loss: 245.2536
Epoch 8/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 248.1592 - val_loss: 241.6171
Epoch 9/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 244.6888 - val_loss: 240.7262
Epoch 10/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 241.6155 - val_loss: 237.0329
Epoch 11/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 239.0664 - val_loss: 232.3177
Epoch 12/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 236.6491 - val_loss: 244.3100
Epoch 13/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 234.7238 - val_loss: 232.5837
Epoch 14/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 232.9833 - val_loss: 226.6758
Epoch 15/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 231.5722 - val_loss: 235.0574
Epoch 16/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 230.4226 - val_loss: 227.1212
Epoch 17/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 229.1113 - val_loss: 224.7039
Epoch 18/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 228.0495 - val_loss: 234.8479
Epoch 19/43
9445953/9445953 [==============================] - 26s 3us/step - loss: 227.1268 - val_loss: 221.9836
Epoch 20/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 226.1557 - val_loss: 220.8219
Epoch 21/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 225.4348 - val_loss: 220.7732
Epoch 22/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 224.6892 - val_loss: 227.0471
Epoch 23/43
9445953/9445953 [==============================] - 24s 2us/step - loss: 223.8657 - val_loss: 228.2073
Epoch 24/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 223.2326 - val_loss: 222.4047
Epoch 25/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 222.7308 - val_loss: 222.4487
Epoch 26/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 222.0203 - val_loss: 225.1217                   Epoch 27/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 221.5302 - val_loss: 234.4838
Epoch 28/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 221.1239 - val_loss: 217.1998                   Epoch 29/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 220.6679 - val_loss: 219.9188
Epoch 30/43                                                                                                             9445953/9445953 [==============================] - 22s 2us/step - loss: 220.0157 - val_loss: 218.2758
Epoch 31/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 219.5867 - val_loss: 217.2769                   Epoch 32/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 219.1109 - val_loss: 216.5872
Epoch 33/43                                                                                                             9445953/9445953 [==============================] - 21s 2us/step - loss: 218.7462 - val_loss: 219.5785
Epoch 34/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 218.4139 - val_loss: 221.0185                   Epoch 35/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 217.9615 - val_loss: 221.2827
Epoch 36/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 217.6964 - val_loss: 218.8342
Epoch 37/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 217.0735 - val_loss: 218.4797
Epoch 38/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 217.0109 - val_loss: 216.2471
Epoch 39/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 216.4645 - val_loss: 216.7753
Epoch 40/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 216.3231 - val_loss: 216.6017
Epoch 41/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 215.7580 - val_loss: 222.1501
Epoch 42/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 215.4877 - val_loss: 216.6402
Epoch 43/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.2718 - val_loss: 221.0634

 ###### inititial_accuracy = 224.39665541004575



 QUEUE SIZE BEFORE GET = 1


 NODE SELECTED <<<------ Epochs = 43 -------- BatchSize = 6400--------- Accuracy = 224.39665541004575 -------->>>

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/40
2018-07-16 14:20:21.466797: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 14:20:21.471970: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 24s 3us/step - loss: 363.8887 - val_loss: 343.0420
Epoch 2/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 334.5092 - val_loss: 323.9149
Epoch 3/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 322.3935 - val_loss: 317.7880
Epoch 4/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.5026 - val_loss: 317.0922
Epoch 5/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3040 - val_loss: 317.0896
Epoch 6/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3031 - val_loss: 317.0899
Epoch 7/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3031 - val_loss: 317.0894
Epoch 8/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0894
Epoch 9/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3031 - val_loss: 317.0908
Epoch 10/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0903
Epoch 11/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3031 - val_loss: 317.0910
Epoch 12/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0897
Epoch 13/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0900
Epoch 14/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3033 - val_loss: 317.0909
Epoch 15/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0894
Epoch 16/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0896
Epoch 17/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0913
Epoch 18/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 19/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 20/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 21/40
9445953/9445953 [==============================] - 25s 3us/step - loss: 319.3033 - val_loss: 317.0902
Epoch 22/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3032 - val_loss: 317.0896
Epoch 23/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0895
Epoch 24/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 25/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0899
Epoch 26/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 27/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 28/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0910
Epoch 29/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3030 - val_loss: 317.0894
Epoch 30/40
9445953/9445953 [==============================] - 26s 3us/step - loss: 319.3031 - val_loss: 317.0943
Epoch 31/40
9445953/9445953 [==============================] - 25s 3us/step - loss: 319.3031 - val_loss: 317.0900
Epoch 32/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 33/40
9445953/9445953 [==============================] - 25s 3us/step - loss: 319.3030 - val_loss: 317.0902
Epoch 34/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3031 - val_loss: 317.0907
Epoch 35/40
9445953/9445953 [==============================] - 24s 2us/step - loss: 319.3031 - val_loss: 317.0902
Epoch 36/40
9445953/9445953 [==============================] - 25s 3us/step - loss: 319.3032 - val_loss: 317.0906
Epoch 37/40
9445953/9445953 [==============================] - 25s 3us/step - loss: 319.3032 - val_loss: 317.0907
Epoch 38/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0893
Epoch 39/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0894
Epoch 40/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3030 - val_loss: 317.0894

 CHILD 1 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 5600

         EPOCHS = 40

         CHILD ACCURACY = 314.87560379744133

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/40
2018-07-16 14:35:56.582279: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 14:35:56.587745: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 22s 2us/step - loss: 309.2759 - val_loss: 284.3210
Epoch 2/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 283.5369 - val_loss: 275.7402
Epoch 3/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 276.0577 - val_loss: 273.7803
Epoch 4/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 272.7992 - val_loss: 268.8324
Epoch 5/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 269.0120 - val_loss: 263.9368
Epoch 6/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 261.4435 - val_loss: 257.7480
Epoch 7/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 255.0742 - val_loss: 258.0353
Epoch 8/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 250.0803 - val_loss: 246.2059
Epoch 9/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 246.0414 - val_loss: 239.9709
Epoch 10/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 242.8274 - val_loss: 237.2206
Epoch 11/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 239.9379 - val_loss: 250.1150
Epoch 12/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 237.8155 - val_loss: 237.9874
Epoch 13/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 235.9240 - val_loss: 237.1158
Epoch 14/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 234.3200 - val_loss: 242.1086
Epoch 15/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 232.7488 - val_loss: 243.2946
Epoch 16/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 231.5883 - val_loss: 239.3045
Epoch 17/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 230.4653 - val_loss: 228.9657
Epoch 18/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 229.6092 - val_loss: 233.5232
Epoch 19/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 228.5309 - val_loss: 231.6801
Epoch 20/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 227.8986 - val_loss: 229.6488
Epoch 21/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 227.1074 - val_loss: 227.0389
Epoch 22/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 226.2899 - val_loss: 225.1427
Epoch 23/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 225.6016 - val_loss: 222.1370
Epoch 24/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 225.0785 - val_loss: 223.1108
Epoch 25/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 224.5723 - val_loss: 232.6523
Epoch 26/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 224.0320 - val_loss: 226.0786
Epoch 27/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 223.4106 - val_loss: 229.7043
Epoch 28/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 223.1263 - val_loss: 224.0151
Epoch 29/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 222.6977 - val_loss: 226.6727
Epoch 30/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 222.2897 - val_loss: 231.8539
Epoch 31/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 221.8472 - val_loss: 221.6752
Epoch 32/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 221.3576 - val_loss: 222.2261
Epoch 33/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 220.9676 - val_loss: 220.9737
Epoch 34/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 220.3638 - val_loss: 223.1675
Epoch 35/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 220.3683 - val_loss: 221.8572
Epoch 36/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 219.7050 - val_loss: 227.0139
Epoch 37/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 219.5287 - val_loss: 218.1620
Epoch 38/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 219.2075 - val_loss: 223.6131
Epoch 39/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 219.0530 - val_loss: 223.9063
Epoch 40/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 218.5543 - val_loss: 219.1977

 CHILD 2 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 7200

         EPOCHS = 40

         CHILD ACCURACY = 225.53974553024617

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/40
2018-07-16 14:50:21.889127: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 14:50:21.895137: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 22s 2us/step - loss: 366.3047 - val_loss: 346.7731
Epoch 2/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 338.2785 - val_loss: 327.2036
Epoch 3/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 324.7210 - val_loss: 319.0457
Epoch 4/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 320.0699 - val_loss: 317.1887
Epoch 5/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3296 - val_loss: 317.0893
Epoch 6/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3031 - val_loss: 317.0905
Epoch 7/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3030 - val_loss: 317.0895
Epoch 8/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3032 - val_loss: 317.0905
Epoch 9/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3031 - val_loss: 317.0909
Epoch 10/40
9445953/9445953 [==============================] - 25s 3us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 11/40
9445953/9445953 [==============================] - 25s 3us/step - loss: 319.3030 - val_loss: 317.0900
Epoch 12/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 13/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3030 - val_loss: 317.0922
Epoch 14/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 15/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3033 - val_loss: 317.0899
Epoch 16/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3033 - val_loss: 317.0897
Epoch 17/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3031 - val_loss: 317.0921
Epoch 18/40
9445953/9445953 [==============================] - 25s 3us/step - loss: 319.3032 - val_loss: 317.0907
Epoch 19/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3031 - val_loss: 317.0901
Epoch 20/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3032 - val_loss: 317.0900
Epoch 21/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3034 - val_loss: 317.0905
Epoch 22/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 23/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3032 - val_loss: 317.0904
Epoch 24/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3032 - val_loss: 317.0902
Epoch 25/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3031 - val_loss: 317.0900
Epoch 26/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3031 - val_loss: 317.0902
Epoch 27/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3032 - val_loss: 317.0913
Epoch 28/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3032 - val_loss: 317.0912
Epoch 29/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0895
Epoch 30/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0916
Epoch 31/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0906
Epoch 32/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0906
Epoch 33/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3030 - val_loss: 317.0903
Epoch 34/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3033 - val_loss: 317.0898
Epoch 35/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0912
Epoch 36/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3031 - val_loss: 317.0906
Epoch 37/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0913
Epoch 38/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 39/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3033 - val_loss: 317.0908
Epoch 40/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0903

 CHILD 3 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 6400

         EPOCHS = 40

         CHILD ACCURACY = 314.87702782998923

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/43
2018-07-16 15:05:17.616143: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 15:05:17.622750: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 21s 2us/step - loss: 312.3632 - val_loss: 292.1479
Epoch 2/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 287.6420 - val_loss: 283.9120
Epoch 3/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 283.0392 - val_loss: 278.9900
Epoch 4/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 279.5670 - val_loss: 276.5177
Epoch 5/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 277.2121 - val_loss: 273.9243
Epoch 6/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 275.6512 - val_loss: 272.2297
Epoch 7/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 272.9933 - val_loss: 269.5355
Epoch 8/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 269.5476 - val_loss: 265.6918
Epoch 9/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 263.6371 - val_loss: 256.3012
Epoch 10/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 257.1692 - val_loss: 253.1782
Epoch 11/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 251.8588 - val_loss: 251.3035
Epoch 12/43
9445953/9445953 [==============================] - 25s 3us/step - loss: 247.6093 - val_loss: 242.7430
Epoch 13/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 244.4130 - val_loss: 238.1375
Epoch 14/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 241.8472 - val_loss: 238.7059
Epoch 15/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 239.5207 - val_loss: 234.7391
Epoch 16/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 237.7287 - val_loss: 233.4929
Epoch 17/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 235.9879 - val_loss: 237.3874
Epoch 18/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 234.7202 - val_loss: 231.9882
Epoch 19/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 233.3582 - val_loss: 233.8651
Epoch 20/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 232.1107 - val_loss: 229.4498
Epoch 21/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 231.1939 - val_loss: 226.3556
Epoch 22/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 230.0873 - val_loss: 231.3616
Epoch 23/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 229.4399 - val_loss: 228.5311
Epoch 24/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 228.7991 - val_loss: 226.5189
Epoch 25/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 227.6953 - val_loss: 226.2666
Epoch 26/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 227.1524 - val_loss: 236.4735
Epoch 27/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 226.5214 - val_loss: 224.7086
Epoch 28/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 226.0226 - val_loss: 226.5751
Epoch 29/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 225.4359 - val_loss: 228.1272
Epoch 30/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 224.9495 - val_loss: 230.4781
Epoch 31/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 224.5076 - val_loss: 232.5513
Epoch 32/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 223.7919 - val_loss: 224.6918
Epoch 33/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 223.4807 - val_loss: 221.0841
Epoch 34/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 223.0823 - val_loss: 223.8435
Epoch 35/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 222.7102 - val_loss: 221.2081
Epoch 36/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 222.3596 - val_loss: 219.7976
Epoch 37/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 221.7456 - val_loss: 220.0096
Epoch 38/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 221.4828 - val_loss: 220.7278
Epoch 39/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 221.2467 - val_loss: 222.5680
Epoch 40/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 220.6732 - val_loss: 223.9055
Epoch 41/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 220.3834 - val_loss: 218.0486
Epoch 42/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 219.9776 - val_loss: 223.3928
Epoch 43/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 219.7428 - val_loss: 218.3389

 CHILD 4 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 7200

         EPOCHS = 43

         CHILD ACCURACY = 219.81897184339084


                 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> APPENDED CHILD TO THE QUEUE | QUEUE SIZE = 1 <<<<<<<<<<<<<<<<<<<<<<<<<<<


                         ********** BEST MODEL SAVED | EPOCHS = 43 | BATCH_SIZE = 7200 | ACCURACY = 219.81897184339084 ********



Train on 9445953 samples, validate on 1049551 samples
Epoch 1/43
2018-07-16 15:20:25.329925: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 15:20:25.336851: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 23s 2us/step - loss: 314.2181 - val_loss: 285.7715
Epoch 2/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 281.7634 - val_loss: 276.0238
Epoch 3/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 275.3719 - val_loss: 268.7323
Epoch 4/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 269.7054 - val_loss: 265.0780
Epoch 5/43
9445953/9445953 [==============================] - 24s 2us/step - loss: 265.7589 - val_loss: 263.2823
Epoch 6/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 262.9045 - val_loss: 259.4960
Epoch 7/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 260.8128 - val_loss: 256.9022
Epoch 8/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 258.6241 - val_loss: 256.9794
Epoch 9/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 254.1888 - val_loss: 248.4286
Epoch 10/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 247.9269 - val_loss: 244.7499
Epoch 11/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 243.2346 - val_loss: 242.6150
Epoch 12/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 239.6327 - val_loss: 233.6643
Epoch 13/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 236.9154 - val_loss: 232.9912
Epoch 14/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 234.7682 - val_loss: 229.3492
Epoch 15/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 232.6784 - val_loss: 228.0901
Epoch 16/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 231.2532 - val_loss: 230.6668
Epoch 17/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 229.9013 - val_loss: 231.9469
Epoch 18/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 228.4396 - val_loss: 247.3250
Epoch 19/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 227.1585 - val_loss: 227.3697
Epoch 20/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 225.8785 - val_loss: 221.9747
Epoch 21/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 225.1197 - val_loss: 232.2378
Epoch 22/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 223.9763 - val_loss: 217.3805
Epoch 23/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 223.2697 - val_loss: 221.2373
Epoch 24/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 222.4707 - val_loss: 217.3272
Epoch 25/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 221.8038 - val_loss: 220.4823
Epoch 26/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 220.8828 - val_loss: 229.8981
Epoch 27/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 220.1849 - val_loss: 217.4915
Epoch 28/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 219.6898 - val_loss: 225.8541
Epoch 29/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 218.8967 - val_loss: 215.4413
Epoch 30/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 218.2486 - val_loss: 219.4062
Epoch 31/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 217.7635 - val_loss: 216.5369
Epoch 32/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 216.9165 - val_loss: 230.5729
Epoch 33/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 216.6875 - val_loss: 219.2133
Epoch 34/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 216.1581 - val_loss: 219.7374
Epoch 35/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 215.5005 - val_loss: 213.4795
Epoch 36/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 215.0421 - val_loss: 212.7671
Epoch 37/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 214.6446 - val_loss: 214.4829
Epoch 38/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 213.9612 - val_loss: 218.1129
Epoch 39/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 213.8201 - val_loss: 213.0426
Epoch 40/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 213.3353 - val_loss: 211.0315
Epoch 41/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 213.0055 - val_loss: 209.3365
Epoch 42/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 212.5091 - val_loss: 214.7113
Epoch 43/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 211.9603 - val_loss: 212.6291

 CHILD 5 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 5600

         EPOCHS = 43

         CHILD ACCURACY = 211.95963977875007


                 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> APPENDED CHILD TO THE QUEUE | QUEUE SIZE = 2 <<<<<<<<<<<<<<<<<<<<<<<<<<<


                         ********** BEST MODEL SAVED | EPOCHS = 43 | BATCH_SIZE = 5600 | ACCURACY = 211.95963977875007 ********



Train on 9445953 samples, validate on 1049551 samples
Epoch 1/46
2018-07-16 15:35:58.536612: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 15:35:58.541648: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 22s 2us/step - loss: 300.5357 - val_loss: 287.0254
Epoch 2/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 283.0398 - val_loss: 279.6159
Epoch 3/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 277.8543 - val_loss: 274.5439
Epoch 4/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 274.3776 - val_loss: 269.2515
Epoch 5/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 268.8569 - val_loss: 263.5941
Epoch 6/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 261.8834 - val_loss: 254.0126
Epoch 7/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 255.6286 - val_loss: 249.8844
Epoch 8/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 250.7622 - val_loss: 244.7526
Epoch 9/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 246.6618 - val_loss: 240.7915
Epoch 10/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 243.3418 - val_loss: 238.1485
Epoch 11/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 240.5369 - val_loss: 237.3501
Epoch 12/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 238.2483 - val_loss: 233.5567
Epoch 13/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 236.1098 - val_loss: 238.0615
Epoch 14/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 234.3499 - val_loss: 232.1425
Epoch 15/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 232.7068 - val_loss: 240.5628
Epoch 16/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 231.2534 - val_loss: 226.6546
Epoch 17/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 230.0352 - val_loss: 227.3866
Epoch 18/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 228.7172 - val_loss: 226.3216
Epoch 19/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 227.8285 - val_loss: 231.2264
Epoch 20/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 226.8845 - val_loss: 226.9335
Epoch 21/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 225.8298 - val_loss: 226.5034
Epoch 22/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 225.0744 - val_loss: 221.8709
Epoch 23/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 224.3619 - val_loss: 225.8278
Epoch 24/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 223.6162 - val_loss: 223.1618
Epoch 25/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 222.9527 - val_loss: 222.4494
Epoch 26/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 222.2158 - val_loss: 231.4686
Epoch 27/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 221.5980 - val_loss: 222.3157
Epoch 28/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 221.0397 - val_loss: 221.7014
Epoch 29/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 220.4598 - val_loss: 226.6480
Epoch 30/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 219.9285 - val_loss: 220.1431
Epoch 31/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 219.6587 - val_loss: 219.9835
Epoch 32/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 218.9148 - val_loss: 222.1951
Epoch 33/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 218.5319 - val_loss: 219.7649
Epoch 34/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 217.9917 - val_loss: 216.4705
Epoch 35/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 217.6133 - val_loss: 219.5667
Epoch 36/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 217.0852 - val_loss: 215.5424
Epoch 37/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 216.7667 - val_loss: 218.2358
Epoch 38/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 216.4861 - val_loss: 218.5078
Epoch 39/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.9475 - val_loss: 213.6468
Epoch 40/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.4919 - val_loss: 218.5932
Epoch 41/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.1032 - val_loss: 221.2344
Epoch 42/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 214.7881 - val_loss: 216.8556
Epoch 43/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 214.5443 - val_loss: 221.7173
Epoch 44/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 214.2045 - val_loss: 212.5038
Epoch 45/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 213.8755 - val_loss: 214.9076
Epoch 46/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 213.5173 - val_loss: 220.0129

 CHILD 6 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 5600

         EPOCHS = 46

         CHILD ACCURACY = 217.75047360479107


                 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> APPENDED CHILD TO THE QUEUE | QUEUE SIZE = 3 <<<<<<<<<<<<<<<<<<<<<<<<<<<

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/46
2018-07-16 15:53:13.495805: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 15:53:13.501553: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 20s 2us/step - loss: 368.2591 - val_loss: 349.8635
Epoch 2/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 341.6199 - val_loss: 330.3821
Epoch 3/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 327.2837 - val_loss: 320.8062
Epoch 4/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 321.0987 - val_loss: 317.5715
Epoch 5/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.4689 - val_loss: 317.0982
Epoch 6/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3056 - val_loss: 317.0904
Epoch 7/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0900
Epoch 8/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 9/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3030 - val_loss: 317.0893
Epoch 10/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0896
Epoch 11/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0926
Epoch 12/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0896
Epoch 13/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3033 - val_loss: 317.0902
Epoch 14/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0925
Epoch 15/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0900
Epoch 16/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0904
Epoch 17/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0900
Epoch 18/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0905
Epoch 19/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 20/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3030 - val_loss: 317.0908
Epoch 21/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0907
Epoch 22/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0917
Epoch 23/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0901
Epoch 24/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0901
Epoch 25/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0902
Epoch 26/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0898
Epoch 27/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 28/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0929
Epoch 29/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0900
Epoch 30/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0903
Epoch 31/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3030 - val_loss: 317.0905
Epoch 32/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0901
Epoch 33/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0913
Epoch 34/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0895
Epoch 35/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0894
Epoch 36/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0909
Epoch 37/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 38/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0902
Epoch 39/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0905
Epoch 40/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0900
Epoch 41/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 42/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3030 - val_loss: 317.0906
Epoch 43/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0904
Epoch 44/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0915
Epoch 45/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 46/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0908

 CHILD 7 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 7200

         EPOCHS = 46

         CHILD ACCURACY = 314.87830145535605

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/46
2018-07-16 16:08:33.545827: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 16:08:33.552504: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 21s 2us/step - loss: 372.7074 - val_loss: 349.0243
Epoch 2/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 340.0673 - val_loss: 328.4835
Epoch 3/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 325.5824 - val_loss: 319.5106
Epoch 4/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 320.2917 - val_loss: 317.2423
Epoch 5/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3435 - val_loss: 317.0893
Epoch 6/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 7/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0897
Epoch 8/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 9/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3031 - val_loss: 317.0894
Epoch 10/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 11/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0895
Epoch 12/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0906
Epoch 13/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 14/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 15/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0914
Epoch 16/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0902
Epoch 17/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0915
Epoch 18/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0895
Epoch 19/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3033 - val_loss: 317.0901
Epoch 20/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0896
Epoch 21/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3030 - val_loss: 317.0902
Epoch 22/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0910
Epoch 23/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0904
Epoch 24/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0906
Epoch 25/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0896
Epoch 26/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0911
Epoch 27/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3033 - val_loss: 317.0914
Epoch 28/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 29/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3033 - val_loss: 317.0907
Epoch 30/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3030 - val_loss: 317.0918
Epoch 31/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3030 - val_loss: 317.0911
Epoch 32/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0906
Epoch 33/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3031 - val_loss: 317.0918
Epoch 34/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0896
Epoch 35/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 36/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0921
Epoch 37/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 38/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3033 - val_loss: 317.0895
Epoch 39/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 40/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0902
Epoch 41/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0899
Epoch 42/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3033 - val_loss: 317.0898
Epoch 43/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 44/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0909
Epoch 45/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0895
Epoch 46/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0893

 CHILD 8 out of 8 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 6400

         EPOCHS = 46

         CHILD ACCURACY = 314.8765913622033



 QUEUE SIZE BEFORE GET = 3


 NODE SELECTED <<<------ Epochs = 43 -------- BatchSize = 5600--------- Accuracy = 211.95963977875007 -------->>>

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/40
2018-07-16 16:24:18.226653: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 16:24:18.232373: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 23s 2us/step - loss: 360.5709 - val_loss: 338.4548
Epoch 2/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 330.3062 - val_loss: 320.7431
Epoch 3/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 320.5748 - val_loss: 317.1878
Epoch 4/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3223 - val_loss: 317.0899
Epoch 5/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3031 - val_loss: 317.0900
Epoch 6/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0914
Epoch 7/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0898
Epoch 8/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3030 - val_loss: 317.0893
Epoch 9/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0893
Epoch 10/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0897
Epoch 11/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0895
Epoch 12/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0898
Epoch 13/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3031 - val_loss: 317.0919
Epoch 14/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0914
Epoch 15/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0908
Epoch 16/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 17/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3032 - val_loss: 317.0911
Epoch 18/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3031 - val_loss: 317.0920
Epoch 19/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3032 - val_loss: 317.0907
Epoch 20/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0919
Epoch 21/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0894
Epoch 22/40
9445953/9445953 [==============================] - 24s 2us/step - loss: 319.3030 - val_loss: 317.0901
Epoch 23/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3031 - val_loss: 317.0892
Epoch 24/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3031 - val_loss: 317.0901
Epoch 25/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0908
Epoch 26/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0909
Epoch 27/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0895
Epoch 28/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0910
Epoch 29/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0901
Epoch 30/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0895
Epoch 31/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0897
Epoch 32/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0904
Epoch 33/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0906
Epoch 34/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0893
Epoch 35/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0920
Epoch 36/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3034 - val_loss: 317.0903
Epoch 37/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 38/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3033 - val_loss: 317.0908
Epoch 39/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 40/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3033 - val_loss: 317.0899

 CHILD 1 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 4800

         EPOCHS = 40

         CHILD ACCURACY = 314.87702104908095

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/40
2018-07-16 16:39:29.066523: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 16:39:29.071377: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 22s 2us/step - loss: 307.2384 - val_loss: 283.9556
Epoch 2/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 282.0884 - val_loss: 274.5855
Epoch 3/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 275.3584 - val_loss: 270.9016
Epoch 4/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 271.3918 - val_loss: 267.3652
Epoch 5/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 264.0576 - val_loss: 265.4755
Epoch 6/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 256.4869 - val_loss: 250.2954
Epoch 7/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 250.9714 - val_loss: 246.0386
Epoch 8/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 246.6226 - val_loss: 242.2968
Epoch 9/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 243.0651 - val_loss: 236.5104
Epoch 10/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 240.0360 - val_loss: 237.3611
Epoch 11/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 237.4831 - val_loss: 237.8015
Epoch 12/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 235.3560 - val_loss: 246.6331
Epoch 13/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 233.4153 - val_loss: 229.1664
Epoch 14/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 231.6509 - val_loss: 230.6876
Epoch 15/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 230.2468 - val_loss: 230.1725
Epoch 16/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 229.0326 - val_loss: 230.9500
Epoch 17/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 227.8059 - val_loss: 226.4998
Epoch 18/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 226.6547 - val_loss: 227.2994
Epoch 19/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 225.6592 - val_loss: 229.8777
Epoch 20/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 224.8851 - val_loss: 226.6453
Epoch 21/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 223.9376 - val_loss: 220.5323
Epoch 22/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 223.2706 - val_loss: 230.2335
Epoch 23/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 222.3664 - val_loss: 220.5750
Epoch 24/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 221.9026 - val_loss: 218.1504
Epoch 25/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 221.1440 - val_loss: 223.0590
Epoch 26/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 220.5602 - val_loss: 225.5378
Epoch 27/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 220.1491 - val_loss: 220.4296
Epoch 28/40
9445953/9445953 [==============================] - 24s 2us/step - loss: 219.4447 - val_loss: 222.2607
Epoch 29/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 218.8240 - val_loss: 216.6016
Epoch 30/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 218.5526 - val_loss: 219.0470
Epoch 31/40
9445953/9445953 [==============================] - 21s 2us/step - loss: 217.9694 - val_loss: 218.5100
Epoch 32/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 217.3406 - val_loss: 220.4235
Epoch 33/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 217.0323 - val_loss: 225.6587
Epoch 34/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 216.4273 - val_loss: 219.4939
Epoch 35/40
9445953/9445953 [==============================] - 22s 2us/step - loss: 215.9461 - val_loss: 213.7213
Epoch 36/40
9445953/9445953 [==============================] - 24s 3us/step - loss: 215.6794 - val_loss: 219.7849
Epoch 37/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.1796 - val_loss: 216.9663
Epoch 38/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 214.7716 - val_loss: 213.9083
Epoch 39/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 214.3090 - val_loss: 212.8736
Epoch 40/40
9445953/9445953 [==============================] - 23s 2us/step - loss: 214.1126 - val_loss: 225.7172

 CHILD 2 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 5600

         EPOCHS = 40

         CHILD ACCURACY = 220.60166608999188

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/43
2018-07-16 16:53:49.963780: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 16:53:49.969510: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 22s 2us/step - loss: 305.5240 - val_loss: 283.7180
Epoch 2/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 282.7733 - val_loss: 277.7126
Epoch 3/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 279.5735 - val_loss: 275.9140
Epoch 4/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 276.9477 - val_loss: 272.7450
Epoch 5/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 272.9590 - val_loss: 267.5325
Epoch 6/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 263.9218 - val_loss: 261.1648
Epoch 7/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 254.5673 - val_loss: 253.2709
Epoch 8/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 248.3773 - val_loss: 247.6676
Epoch 9/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 243.9348 - val_loss: 243.5674
Epoch 10/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 239.9299 - val_loss: 234.4776
Epoch 11/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 236.6342 - val_loss: 238.4524
Epoch 12/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 233.6693 - val_loss: 233.2810
Epoch 13/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 231.1659 - val_loss: 230.4393
Epoch 14/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 229.0924 - val_loss: 245.8725
Epoch 15/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 227.3909 - val_loss: 234.1843
Epoch 16/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 225.7637 - val_loss: 220.3376
Epoch 17/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 224.2368 - val_loss: 225.4382
Epoch 18/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 223.0163 - val_loss: 222.1289
Epoch 19/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 221.8554 - val_loss: 216.5974
Epoch 20/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 220.7069 - val_loss: 224.4001
Epoch 21/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 219.9445 - val_loss: 223.1586
Epoch 22/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 219.0444 - val_loss: 214.4589
Epoch 23/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 218.1017 - val_loss: 214.7573
Epoch 24/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 217.3129 - val_loss: 221.5253
Epoch 25/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 216.6273 - val_loss: 222.1881
Epoch 26/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 215.8014 - val_loss: 211.8231
Epoch 27/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 215.0375 - val_loss: 212.8557
Epoch 28/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 214.2753 - val_loss: 219.2806
Epoch 29/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 213.7764 - val_loss: 216.9531
Epoch 30/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 213.1907 - val_loss: 209.7899
Epoch 31/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 212.7052 - val_loss: 218.0915
Epoch 32/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 212.1662 - val_loss: 209.2847
Epoch 33/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 211.7778 - val_loss: 213.9712
Epoch 34/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 210.9597 - val_loss: 228.7253
Epoch 35/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 210.6648 - val_loss: 211.3667
Epoch 36/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 210.2333 - val_loss: 210.0147
Epoch 37/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 209.6787 - val_loss: 207.7418
Epoch 38/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 209.3808 - val_loss: 207.5228
Epoch 39/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 208.8104 - val_loss: 209.1245
Epoch 40/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 208.3161 - val_loss: 209.4014
Epoch 41/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 207.8167 - val_loss: 208.4793
Epoch 42/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 207.5107 - val_loss: 212.1819
Epoch 43/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 207.0899 - val_loss: 208.9771

 CHILD 3 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 6400

         EPOCHS = 43

         CHILD ACCURACY = 212.28637823716625

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/43
2018-07-16 17:08:57.849684: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 17:08:57.854670: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 23s 2us/step - loss: 296.6116 - val_loss: 280.8355
Epoch 2/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 281.8396 - val_loss: 282.8940
Epoch 3/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 277.1349 - val_loss: 272.2640
Epoch 4/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 269.7535 - val_loss: 266.2510
Epoch 5/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 260.9286 - val_loss: 255.0123
Epoch 6/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 253.3589 - val_loss: 246.7812
Epoch 7/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 247.6515 - val_loss: 246.2929
Epoch 8/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 243.1765 - val_loss: 239.9211
Epoch 9/43
9445953/9445953 [==============================] - 25s 3us/step - loss: 239.5391 - val_loss: 235.3527
Epoch 10/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 236.7334 - val_loss: 232.7578
Epoch 11/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 234.2688 - val_loss: 232.7445
Epoch 12/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 232.3765 - val_loss: 231.0130
Epoch 13/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 230.6540 - val_loss: 236.3539
Epoch 14/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 228.9925 - val_loss: 226.0720
Epoch 15/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 227.7106 - val_loss: 223.8178
Epoch 16/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 226.3863 - val_loss: 226.3706
Epoch 17/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 225.3115 - val_loss: 229.5412
Epoch 18/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 224.2941 - val_loss: 227.8409
Epoch 19/43
9445953/9445953 [==============================] - 26s 3us/step - loss: 223.4106 - val_loss: 227.1827
Epoch 20/43
9445953/9445953 [==============================] - 25s 3us/step - loss: 222.5091 - val_loss: 223.6698
Epoch 21/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 221.6583 - val_loss: 218.6650
Epoch 22/43
9445953/9445953 [==============================] - 24s 2us/step - loss: 220.9192 - val_loss: 220.8720
Epoch 23/43
9445953/9445953 [==============================] - 25s 3us/step - loss: 220.0866 - val_loss: 221.3322
Epoch 24/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 219.6171 - val_loss: 219.1738
Epoch 25/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 218.8869 - val_loss: 224.1104
Epoch 26/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 218.2018 - val_loss: 227.2564
Epoch 27/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 217.6937 - val_loss: 213.7834
Epoch 28/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 217.0653 - val_loss: 217.8196
Epoch 29/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 216.3126 - val_loss: 223.5468
Epoch 30/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 216.1757 - val_loss: 214.0152
Epoch 31/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.4363 - val_loss: 215.6582
Epoch 32/43
9445953/9445953 [==============================] - 23s 2us/step - loss: 214.8693 - val_loss: 223.0131
Epoch 33/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 214.7019 - val_loss: 214.8773
Epoch 34/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 214.1197 - val_loss: 212.5439
Epoch 35/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 213.6708 - val_loss: 214.7293
Epoch 36/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 213.2557 - val_loss: 214.3393
Epoch 37/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 212.6150 - val_loss: 219.9406
Epoch 38/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 212.3261 - val_loss: 219.5315
Epoch 39/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 211.9978 - val_loss: 217.5181
Epoch 40/43
9445953/9445953 [==============================] - 24s 3us/step - loss: 211.7505 - val_loss: 225.0103
Epoch 41/43
9445953/9445953 [==============================] - 26s 3us/step - loss: 211.3593 - val_loss: 212.9890
Epoch 42/43
9445953/9445953 [==============================] - 26s 3us/step - loss: 210.8164 - val_loss: 210.9441
Epoch 43/43
9445953/9445953 [==============================] - 25s 3us/step - loss: 210.4312 - val_loss: 212.2824

 CHILD 4 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 4800

         EPOCHS = 43

         CHILD ACCURACY = 213.27471740357177

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/46
2018-07-16 17:25:54.954200: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 17:25:54.960000: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 25s 3us/step - loss: 305.2479 - val_loss: 281.8764
Epoch 2/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 279.8922 - val_loss: 273.2678
Epoch 3/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 275.2809 - val_loss: 272.1534
Epoch 4/46
9445953/9445953 [==============================] - 24s 2us/step - loss: 273.1330 - val_loss: 272.1350
Epoch 5/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 271.9358 - val_loss: 269.7442
Epoch 6/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 271.1617 - val_loss: 271.7517
Epoch 7/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 270.5420 - val_loss: 270.0095
Epoch 8/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 270.1255 - val_loss: 268.5755
Epoch 9/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 269.7482 - val_loss: 267.8725
Epoch 10/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 269.4076 - val_loss: 267.3928
Epoch 11/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 269.2296 - val_loss: 269.3876
Epoch 12/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 268.9539 - val_loss: 266.9252
Epoch 13/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 268.7034 - val_loss: 266.4494
Epoch 14/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 268.5720 - val_loss: 266.9052
Epoch 15/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 268.3848 - val_loss: 266.6424
Epoch 16/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 268.1653 - val_loss: 265.8185
Epoch 17/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 268.0730 - val_loss: 265.8581
Epoch 18/46
9445953/9445953 [==============================] - 27s 3us/step - loss: 267.9187 - val_loss: 266.7970
Epoch 19/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 267.7493 - val_loss: 265.8956
Epoch 20/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 267.6463 - val_loss: 266.2657
Epoch 21/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 267.5845 - val_loss: 267.1429
Epoch 22/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 267.3650 - val_loss: 265.7287
Epoch 23/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 267.2939 - val_loss: 265.2575
Epoch 24/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 267.1282 - val_loss: 265.5097
Epoch 25/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 266.9733 - val_loss: 266.6413
Epoch 26/46
9445953/9445953 [==============================] - 24s 2us/step - loss: 266.8796 - val_loss: 266.9393
Epoch 27/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 266.8085 - val_loss: 265.0531
Epoch 28/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 266.5973 - val_loss: 265.2904
Epoch 29/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 266.4574 - val_loss: 265.4758
Epoch 30/46
9445953/9445953 [==============================] - 24s 2us/step - loss: 266.3668 - val_loss: 265.7762
Epoch 31/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 266.2407 - val_loss: 265.7504
Epoch 32/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 266.0340 - val_loss: 266.2152
Epoch 33/46
9445953/9445953 [==============================] - 24s 2us/step - loss: 265.9713 - val_loss: 264.7898
Epoch 34/46
9445953/9445953 [==============================] - 24s 2us/step - loss: 265.7346 - val_loss: 265.3740
Epoch 35/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 265.6889 - val_loss: 265.7936
Epoch 36/46
9445953/9445953 [==============================] - 24s 2us/step - loss: 265.4945 - val_loss: 266.3570
Epoch 37/46
9445953/9445953 [==============================] - 24s 2us/step - loss: 265.4166 - val_loss: 264.2325
Epoch 38/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 265.2994 - val_loss: 265.2201
Epoch 39/46
9445953/9445953 [==============================] - 24s 2us/step - loss: 265.1339 - val_loss: 264.0216
Epoch 40/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 264.9155 - val_loss: 265.2531
Epoch 41/46
9445953/9445953 [==============================] - 24s 2us/step - loss: 264.9233 - val_loss: 265.8371
Epoch 42/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 264.7152 - val_loss: 264.0433
Epoch 43/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 264.5532 - val_loss: 262.8423
Epoch 44/46
9445953/9445953 [==============================] - 24s 2us/step - loss: 264.4235 - val_loss: 263.4774
Epoch 45/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 264.1901 - val_loss: 263.0422
Epoch 46/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 264.1706 - val_loss: 263.7715

 CHILD 5 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 4800

         EPOCHS = 46

         CHILD ACCURACY = 262.5992617594031



 QUEUE SIZE BEFORE GET = 2


 NODE SELECTED <<<------ Epochs = 46 -------- BatchSize = 5600--------- Accuracy = 217.75047360479107 -------->>>

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/49
2018-07-16 17:44:18.865962: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 17:44:18.870871: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 21s 2us/step - loss: 310.1495 - val_loss: 289.0133
Epoch 2/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 282.5361 - val_loss: 277.1475
Epoch 3/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 277.1896 - val_loss: 271.9021
Epoch 4/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 272.6476 - val_loss: 267.8017
Epoch 5/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 264.4120 - val_loss: 268.3957
Epoch 6/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 257.2851 - val_loss: 254.1214
Epoch 7/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 251.7673 - val_loss: 258.3485
Epoch 8/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 247.2198 - val_loss: 250.0056
Epoch 9/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 243.0775 - val_loss: 239.0937
Epoch 10/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 239.9567 - val_loss: 240.9928
Epoch 11/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 237.1722 - val_loss: 233.5900
Epoch 12/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 234.8215 - val_loss: 237.0811
Epoch 13/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 232.8668 - val_loss: 228.6897
Epoch 14/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 231.2518 - val_loss: 234.3101
Epoch 15/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 229.6448 - val_loss: 227.3389
Epoch 16/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 228.1309 - val_loss: 232.5895
Epoch 17/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 226.9961 - val_loss: 223.7490
Epoch 18/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 225.7640 - val_loss: 222.4112
Epoch 19/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 224.9034 - val_loss: 232.6048
Epoch 20/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 223.7361 - val_loss: 223.4321
Epoch 21/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 222.8631 - val_loss: 226.8637
Epoch 22/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 222.0161 - val_loss: 222.1273
Epoch 23/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 221.3777 - val_loss: 219.3960
Epoch 24/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 220.6069 - val_loss: 225.4643
Epoch 25/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 219.9763 - val_loss: 218.5952
Epoch 26/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 219.2901 - val_loss: 221.1529
Epoch 27/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 218.5204 - val_loss: 236.8308
Epoch 28/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 218.0892 - val_loss: 222.8151
Epoch 29/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 217.4685 - val_loss: 216.2463
Epoch 30/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 216.9392 - val_loss: 219.2816
Epoch 31/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 216.4517 - val_loss: 212.2174
Epoch 32/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 216.0855 - val_loss: 217.1613
Epoch 33/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 215.5040 - val_loss: 222.9940
Epoch 34/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 214.9219 - val_loss: 214.7944
Epoch 35/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 214.9310 - val_loss: 227.3524
Epoch 36/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 214.1989 - val_loss: 214.8533
Epoch 37/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 213.9148 - val_loss: 214.5884
Epoch 38/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 213.5863 - val_loss: 211.5319
Epoch 39/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 213.0934 - val_loss: 209.7336
Epoch 40/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 212.9464 - val_loss: 223.6560
Epoch 41/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 212.4078 - val_loss: 217.4518
Epoch 42/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 212.3357 - val_loss: 213.5204
Epoch 43/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 211.7752 - val_loss: 210.1548
Epoch 44/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 211.5422 - val_loss: 210.5888
Epoch 45/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 211.0688 - val_loss: 215.1831
Epoch 46/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 210.8272 - val_loss: 209.3594
Epoch 47/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 210.4046 - val_loss: 215.2479
Epoch 48/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 210.5299 - val_loss: 220.8177
Epoch 49/49
9445953/9445953 [==============================] - 20s 2us/step - loss: 210.1012 - val_loss: 207.8638

 CHILD 1 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 6400

         EPOCHS = 49

         CHILD ACCURACY = 218.94107079922682

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/49
2018-07-16 18:00:53.006413: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 18:00:53.011304: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 23s 2us/step - loss: 299.4956 - val_loss: 282.8350
Epoch 2/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 279.3152 - val_loss: 275.3495
Epoch 3/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 277.0681 - val_loss: 273.9359
Epoch 4/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 275.5195 - val_loss: 272.1536
Epoch 5/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 271.0558 - val_loss: 266.4686
Epoch 6/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 264.8498 - val_loss: 260.6814
Epoch 7/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 261.0546 - val_loss: 258.1305
Epoch 8/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 258.0691 - val_loss: 252.7403
Epoch 9/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 255.3739 - val_loss: 251.6236
Epoch 10/49
9445953/9445953 [==============================] - 26s 3us/step - loss: 253.4063 - val_loss: 249.1883
Epoch 11/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 251.7418 - val_loss: 250.7805
Epoch 12/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 250.3856 - val_loss: 254.1579
Epoch 13/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 249.3412 - val_loss: 246.5801
Epoch 14/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 248.3748 - val_loss: 246.6435
Epoch 15/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 247.6304 - val_loss: 247.5144
Epoch 16/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 246.7425 - val_loss: 247.1095
Epoch 17/49
9445953/9445953 [==============================] - 24s 2us/step - loss: 245.9201 - val_loss: 244.0763
Epoch 18/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 245.2645 - val_loss: 242.8188
Epoch 19/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 244.5830 - val_loss: 243.7731
Epoch 20/49
9445953/9445953 [==============================] - 24s 2us/step - loss: 243.9871 - val_loss: 245.2994
Epoch 21/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 243.4041 - val_loss: 241.6479
Epoch 22/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 242.7456 - val_loss: 242.9334
Epoch 23/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 242.1871 - val_loss: 242.4739
Epoch 24/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 241.6205 - val_loss: 239.5983
Epoch 25/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 240.9803 - val_loss: 240.3591
Epoch 26/49
9445953/9445953 [==============================] - 24s 2us/step - loss: 239.8434 - val_loss: 239.9147
Epoch 27/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 233.2663 - val_loss: 228.7432
Epoch 28/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 228.8839 - val_loss: 226.7146
Epoch 29/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 226.3481 - val_loss: 227.2432
Epoch 30/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 224.4353 - val_loss: 220.8819
Epoch 31/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 222.9022 - val_loss: 221.8932
Epoch 32/49
9445953/9445953 [==============================] - 24s 2us/step - loss: 221.8477 - val_loss: 217.9209
Epoch 33/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 220.7903 - val_loss: 222.0044
Epoch 34/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 219.9799 - val_loss: 217.4431
Epoch 35/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 218.8401 - val_loss: 219.3220
Epoch 36/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 218.1478 - val_loss: 217.6513
Epoch 37/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 217.5755 - val_loss: 215.0081
Epoch 38/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 216.9155 - val_loss: 214.9652
Epoch 39/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 216.4232 - val_loss: 214.5835
Epoch 40/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.7927 - val_loss: 217.9748
Epoch 41/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.2842 - val_loss: 215.2243
Epoch 42/49
9445953/9445953 [==============================] - 24s 2us/step - loss: 214.8314 - val_loss: 221.5009
Epoch 43/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 214.3729 - val_loss: 210.6607
Epoch 44/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 213.9497 - val_loss: 216.8443
Epoch 45/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 213.6075 - val_loss: 211.5958
Epoch 46/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 213.2162 - val_loss: 211.2054
Epoch 47/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 212.9350 - val_loss: 211.5354
Epoch 48/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 212.5292 - val_loss: 209.2466
Epoch 49/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 212.1673 - val_loss: 222.4808

 CHILD 2 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 4800

         EPOCHS = 49

         CHILD ACCURACY = 216.29801994321994


                 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> APPENDED CHILD TO THE QUEUE | QUEUE SIZE = 2 <<<<<<<<<<<<<<<<<<<<<<<<<<<

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/49
2018-07-16 18:20:20.705418: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 18:20:20.711003: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 22s 2us/step - loss: 300.1287 - val_loss: 279.2152
Epoch 2/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 278.7808 - val_loss: 275.7906
Epoch 3/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 275.3635 - val_loss: 272.8082
Epoch 4/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 273.4699 - val_loss: 271.0934
Epoch 5/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 272.4273 - val_loss: 272.2390
Epoch 6/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 271.6887 - val_loss: 269.0565
Epoch 7/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 271.2107 - val_loss: 269.0410
Epoch 8/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 270.6716 - val_loss: 268.8159
Epoch 9/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 270.2994 - val_loss: 268.9947
Epoch 10/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 269.8355 - val_loss: 269.7316
Epoch 11/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 269.4426 - val_loss: 267.8011
Epoch 12/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 269.0889 - val_loss: 267.7690
Epoch 13/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 268.6613 - val_loss: 267.0932
Epoch 14/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 268.2528 - val_loss: 265.6872
Epoch 15/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 267.6704 - val_loss: 265.5290
Epoch 16/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 266.4848 - val_loss: 262.6324
Epoch 17/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 261.6630 - val_loss: 255.8776
Epoch 18/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 252.3827 - val_loss: 250.6503
Epoch 19/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 245.8643 - val_loss: 244.0400
Epoch 20/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 241.5467 - val_loss: 237.5492
Epoch 21/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 238.1990 - val_loss: 235.3574
Epoch 22/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 235.5682 - val_loss: 239.9208
Epoch 23/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 233.3231 - val_loss: 234.0032
Epoch 24/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 231.8670 - val_loss: 228.1609
Epoch 25/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 230.5273 - val_loss: 230.5136
Epoch 26/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 229.4519 - val_loss: 227.1431
Epoch 27/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 228.4183 - val_loss: 230.1356
Epoch 28/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 227.6180 - val_loss: 226.3549
Epoch 29/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 226.8643 - val_loss: 225.6477
Epoch 30/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 226.2266 - val_loss: 223.7474
Epoch 31/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 225.5573 - val_loss: 231.3612
Epoch 32/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 224.8109 - val_loss: 226.4597
Epoch 33/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 224.3295 - val_loss: 223.4447
Epoch 34/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 223.6462 - val_loss: 221.6181
Epoch 35/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 223.2273 - val_loss: 219.4850
Epoch 36/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 222.5255 - val_loss: 226.2051
Epoch 37/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 222.2232 - val_loss: 219.1345
Epoch 38/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 221.7749 - val_loss: 219.9448
Epoch 39/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 221.2324 - val_loss: 218.2959
Epoch 40/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 221.0482 - val_loss: 222.7965
Epoch 41/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 220.5798 - val_loss: 235.7527
Epoch 42/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 220.1754 - val_loss: 221.0087
Epoch 43/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 219.8273 - val_loss: 217.8011
Epoch 44/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 219.4776 - val_loss: 218.4123
Epoch 45/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 219.2638 - val_loss: 218.8942
Epoch 46/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 218.8041 - val_loss: 222.7937
Epoch 47/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 218.4966 - val_loss: 219.9727
Epoch 48/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 218.1302 - val_loss: 219.1318
Epoch 49/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 217.9020 - val_loss: 216.4495

 CHILD 3 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 5600

         EPOCHS = 49

         CHILD ACCURACY = 218.85975979579607

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/43
2018-07-16 18:38:27.347560: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 18:38:27.353534: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 21s 2us/step - loss: 365.7094 - val_loss: 346.4851
Epoch 2/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 338.0554 - val_loss: 327.0452
Epoch 3/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 324.6240 - val_loss: 319.0004
Epoch 4/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 320.0471 - val_loss: 317.1830
Epoch 5/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3281 - val_loss: 317.0894
Epoch 6/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0902
Epoch 7/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0905
Epoch 8/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0900
Epoch 9/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0896
Epoch 10/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0911
Epoch 11/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0908
Epoch 12/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0901
Epoch 13/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0907
Epoch 14/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 15/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0894
Epoch 16/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 17/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 18/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3031 - val_loss: 317.0904
Epoch 19/43
9445953/9445953 [==============================] - 21s 2us/step - loss: 319.3032 - val_loss: 317.0917
Epoch 20/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 21/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3029 - val_loss: 317.0894
Epoch 22/43
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3033 - val_loss: 317.0901
Epoch 23/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0927
Epoch 24/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0909
Epoch 25/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 26/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 27/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 28/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0909
Epoch 29/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3029 - val_loss: 317.0934
Epoch 30/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0904
Epoch 31/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0925
Epoch 32/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0901
Epoch 33/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 34/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 35/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0896
Epoch 36/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0895
Epoch 37/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0900
Epoch 38/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0905
Epoch 39/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0912
Epoch 40/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0910
Epoch 41/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 42/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0896
Epoch 43/43
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0893

 CHILD 4 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 6400

         EPOCHS = 43

         CHILD ACCURACY = 314.876321229451

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/46
2018-07-16 18:53:04.188169: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 18:53:04.194619: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 23s 2us/step - loss: 361.1887 - val_loss: 338.7508
Epoch 2/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 330.5095 - val_loss: 320.8488
Epoch 3/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 320.6176 - val_loss: 317.1954
Epoch 4/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3242 - val_loss: 317.0894
Epoch 5/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3031 - val_loss: 317.0912
Epoch 6/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0911
Epoch 7/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0914
Epoch 8/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0907
Epoch 9/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0900
Epoch 10/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0912
Epoch 11/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 319.3033 - val_loss: 317.0900
Epoch 12/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3033 - val_loss: 317.0895
Epoch 13/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0915
Epoch 14/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0907
Epoch 15/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0894
Epoch 16/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0902
Epoch 17/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3033 - val_loss: 317.0897
Epoch 18/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3033 - val_loss: 317.0896
Epoch 19/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0909
Epoch 20/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3029 - val_loss: 317.0929
Epoch 21/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3033 - val_loss: 317.0912
Epoch 22/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3032 - val_loss: 317.0900
Epoch 23/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0906
Epoch 24/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0898
Epoch 25/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3033 - val_loss: 317.0906
Epoch 26/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 27/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3031 - val_loss: 317.0896
Epoch 28/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0898
Epoch 29/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3033 - val_loss: 317.0894
Epoch 30/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0913
Epoch 31/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0901
Epoch 32/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3031 - val_loss: 317.0924
Epoch 33/46
9445953/9445953 [==============================] - 24s 3us/step - loss: 319.3031 - val_loss: 317.0906
Epoch 34/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 35/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3033 - val_loss: 317.0909
Epoch 36/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 319.3032 - val_loss: 317.0894
Epoch 37/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0900
Epoch 38/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0900
Epoch 39/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0902
Epoch 40/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0894
Epoch 41/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3033 - val_loss: 317.0895
Epoch 42/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 43/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0905
Epoch 44/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3031 - val_loss: 317.0905
Epoch 45/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 46/46
9445953/9445953 [==============================] - 23s 2us/step - loss: 319.3032 - val_loss: 317.0897

 CHILD 5 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 4800

         EPOCHS = 46

         CHILD ACCURACY = 314.87681740245165



 QUEUE SIZE BEFORE GET = 2


 NODE SELECTED <<<------ Epochs = 49 -------- BatchSize = 4800--------- Accuracy = 216.29801994321994 -------->>>

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/49
2018-07-16 19:10:37.511750: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 19:10:37.518743: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 26s 3us/step - loss: 293.6398 - val_loss: 277.0410
Epoch 2/49
9445953/9445953 [==============================] - 29s 3us/step - loss: 278.4665 - val_loss: 274.7825
Epoch 3/49
9445953/9445953 [==============================] - 26s 3us/step - loss: 276.7413 - val_loss: 275.6582
Epoch 4/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 275.9603 - val_loss: 274.5363
Epoch 5/49
9445953/9445953 [==============================] - 26s 3us/step - loss: 275.4824 - val_loss: 273.0951
Epoch 6/49
9445953/9445953 [==============================] - 26s 3us/step - loss: 275.1549 - val_loss: 272.6832
Epoch 7/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 274.8904 - val_loss: 273.5771
Epoch 8/49
9445953/9445953 [==============================] - 28s 3us/step - loss: 274.6697 - val_loss: 273.9355
Epoch 9/49
9445953/9445953 [==============================] - 26s 3us/step - loss: 274.3973 - val_loss: 272.4191
Epoch 10/49
9445953/9445953 [==============================] - 26s 3us/step - loss: 274.0224 - val_loss: 273.8967
Epoch 11/49
9445953/9445953 [==============================] - 26s 3us/step - loss: 273.3859 - val_loss: 270.3067
Epoch 12/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 267.9804 - val_loss: 262.9089
Epoch 13/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 262.5132 - val_loss: 261.9366
Epoch 14/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 259.4866 - val_loss: 258.0832
Epoch 15/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 257.4423 - val_loss: 256.7352
Epoch 16/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 256.0574 - val_loss: 254.3325
Epoch 17/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 254.9316 - val_loss: 252.4586
Epoch 18/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 253.9701 - val_loss: 252.0441
Epoch 19/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 253.0456 - val_loss: 252.2432
Epoch 20/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 252.1777 - val_loss: 252.1408
Epoch 21/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 251.4084 - val_loss: 248.7226
Epoch 22/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 250.6772 - val_loss: 248.2178
Epoch 23/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 250.0090 - val_loss: 253.6012
Epoch 24/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 249.4241 - val_loss: 250.5329
Epoch 25/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 249.0089 - val_loss: 247.6587
Epoch 26/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 248.4643 - val_loss: 247.4811
Epoch 27/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 248.0887 - val_loss: 256.3968
Epoch 28/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 247.7447 - val_loss: 254.5079
Epoch 29/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 247.3150 - val_loss: 246.0517
Epoch 30/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 247.0375 - val_loss: 245.7220
Epoch 31/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 246.7075 - val_loss: 247.3272
Epoch 32/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 246.4419 - val_loss: 245.2402
Epoch 33/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 246.1054 - val_loss: 244.7402
Epoch 34/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 245.8362 - val_loss: 248.7797
Epoch 35/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 245.0526 - val_loss: 244.7941
Epoch 36/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 244.1318 - val_loss: 244.0023
Epoch 37/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 243.6805 - val_loss: 242.5957
Epoch 38/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 243.1525 - val_loss: 243.3022
Epoch 39/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 242.3263 - val_loss: 244.2522
Epoch 40/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 240.1012 - val_loss: 237.4849
Epoch 41/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 237.2844 - val_loss: 235.2514
Epoch 42/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 234.6594 - val_loss: 242.8004
Epoch 43/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 232.1075 - val_loss: 229.3966
Epoch 44/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 229.7686 - val_loss: 227.1143
Epoch 45/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 227.8838 - val_loss: 240.2246
Epoch 46/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 226.3913 - val_loss: 232.1504
Epoch 47/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 225.0019 - val_loss: 223.8455
Epoch 48/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 223.8707 - val_loss: 234.0782
Epoch 49/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 223.0387 - val_loss: 230.7731

 CHILD 1 out of 6 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 4000

         EPOCHS = 49

         CHILD ACCURACY = 235.1607772088316

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/49
2018-07-16 19:31:08.100257: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 19:31:08.105116: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 21s 2us/step - loss: 299.2181 - val_loss: 280.7291
Epoch 2/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 280.1682 - val_loss: 276.6784
Epoch 3/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 275.7851 - val_loss: 272.3127
Epoch 4/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 273.2178 - val_loss: 270.3405
Epoch 5/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 269.9738 - val_loss: 266.7377
Epoch 6/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 260.7990 - val_loss: 253.4701
Epoch 7/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 253.5674 - val_loss: 248.4305
Epoch 8/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 248.1186 - val_loss: 242.8837
Epoch 9/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 243.7141 - val_loss: 237.5660
Epoch 10/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 240.1081 - val_loss: 235.1871
Epoch 11/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 237.1698 - val_loss: 235.5097
Epoch 12/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 234.7337 - val_loss: 229.9660
Epoch 13/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 232.7797 - val_loss: 234.1705
Epoch 14/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 231.0879 - val_loss: 226.8463
Epoch 15/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 229.6702 - val_loss: 227.8496
Epoch 16/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 228.4872 - val_loss: 228.9472
Epoch 17/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 227.2173 - val_loss: 223.6199
Epoch 18/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 226.2703 - val_loss: 223.5625
Epoch 19/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 225.1819 - val_loss: 226.0945
Epoch 20/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 224.6183 - val_loss: 224.1284
Epoch 21/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 223.7495 - val_loss: 220.1498
Epoch 22/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 223.0653 - val_loss: 220.3265
Epoch 23/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 222.4570 - val_loss: 221.1858
Epoch 24/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 221.9061 - val_loss: 225.1928
Epoch 25/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 221.2555 - val_loss: 219.4848
Epoch 26/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 220.6813 - val_loss: 222.2759
Epoch 27/49
9445953/9445953 [==============================] - 21s 2us/step - loss: 220.2475 - val_loss: 219.3599
Epoch 28/49
9445953/9445953 [==============================] - 28s 3us/step - loss: 219.6819 - val_loss: 218.9671
Epoch 29/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 219.3227 - val_loss: 221.8922
Epoch 30/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 218.6997 - val_loss: 218.1160
Epoch 31/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 218.1943 - val_loss: 216.3705
Epoch 32/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 218.0559 - val_loss: 221.2463
Epoch 33/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 217.4940 - val_loss: 217.4588
Epoch 34/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 217.2008 - val_loss: 223.3733
Epoch 35/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 216.9384 - val_loss: 215.2222
Epoch 36/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 216.3951 - val_loss: 221.8071
Epoch 37/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 216.0333 - val_loss: 216.2413
Epoch 38/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 215.7980 - val_loss: 218.7045
Epoch 39/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 215.5836 - val_loss: 225.4696
Epoch 40/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 215.2511 - val_loss: 238.7020
Epoch 41/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 214.9967 - val_loss: 222.3554
Epoch 42/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 214.7012 - val_loss: 223.8845
Epoch 43/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 214.4798 - val_loss: 217.3373
Epoch 44/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 213.9483 - val_loss: 213.8080
Epoch 45/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 213.9716 - val_loss: 213.0386
Epoch 46/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 213.6203 - val_loss: 228.0609
Epoch 47/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 213.3495 - val_loss: 213.0802
Epoch 48/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 213.1741 - val_loss: 229.5687
Epoch 49/49
9445953/9445953 [==============================] - 22s 2us/step - loss: 212.8554 - val_loss: 211.6232

 CHILD 2 out of 6 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 5600

         EPOCHS = 49

         CHILD ACCURACY = 223.05508767216315

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/52
2018-07-16 19:48:57.788581: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 19:48:57.793739: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 28s 3us/step - loss: 298.2327 - val_loss: 277.9158
Epoch 2/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 275.4098 - val_loss: 272.1264
Epoch 3/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 267.3497 - val_loss: 260.5554
Epoch 4/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 258.4677 - val_loss: 266.3275
Epoch 5/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 251.9669 - val_loss: 248.9894
Epoch 6/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 247.3187 - val_loss: 245.8989
Epoch 7/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 243.4049 - val_loss: 241.8651
Epoch 8/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 240.1275 - val_loss: 238.1056
Epoch 9/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 237.7574 - val_loss: 244.5585
Epoch 10/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 235.4942 - val_loss: 230.3280
Epoch 11/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 233.7599 - val_loss: 232.0232
Epoch 12/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 232.1391 - val_loss: 238.2837
Epoch 13/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 230.7981 - val_loss: 258.3020
Epoch 14/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 229.5302 - val_loss: 230.1242
Epoch 15/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 228.2972 - val_loss: 234.5952
Epoch 16/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 227.3725 - val_loss: 229.3498
Epoch 17/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 226.4620 - val_loss: 231.2768
Epoch 18/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 225.5717 - val_loss: 224.2368
Epoch 19/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 224.8085 - val_loss: 225.4219
Epoch 20/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 224.1506 - val_loss: 227.5018
Epoch 21/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 223.3654 - val_loss: 223.5541
Epoch 22/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 222.7676 - val_loss: 242.2175
Epoch 23/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 222.1029 - val_loss: 222.4178
Epoch 24/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 221.6004 - val_loss: 220.9382
Epoch 25/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 221.1606 - val_loss: 244.9598
Epoch 26/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 220.5729 - val_loss: 221.2027
Epoch 27/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 220.3031 - val_loss: 218.5718
Epoch 28/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 219.6837 - val_loss: 225.9672
Epoch 29/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 219.1905 - val_loss: 219.2740
Epoch 30/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 218.8565 - val_loss: 226.2734
Epoch 31/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 218.3680 - val_loss: 219.6037
Epoch 32/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 218.0563 - val_loss: 238.8435
Epoch 33/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 217.6987 - val_loss: 225.0002
Epoch 34/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 217.3945 - val_loss: 219.4195
Epoch 35/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 216.9351 - val_loss: 220.5407
Epoch 36/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 216.6700 - val_loss: 219.2087
Epoch 37/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 216.2879 - val_loss: 215.5583
Epoch 38/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 216.0475 - val_loss: 236.4260
Epoch 39/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 215.5769 - val_loss: 218.3105
Epoch 40/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 215.4024 - val_loss: 221.5349
Epoch 41/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 215.1227 - val_loss: 216.5749
Epoch 42/52
9445953/9445953 [==============================] - 29s 3us/step - loss: 214.8169 - val_loss: 215.9894
Epoch 43/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 214.7367 - val_loss: 216.6379
Epoch 44/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 214.2593 - val_loss: 216.3832
Epoch 45/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 214.0626 - val_loss: 227.0321
Epoch 46/52
9445953/9445953 [==============================] - 29s 3us/step - loss: 213.7769 - val_loss: 216.0875
Epoch 47/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 213.5947 - val_loss: 217.3591
Epoch 48/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 213.4079 - val_loss: 217.2998
Epoch 49/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 213.0231 - val_loss: 241.2621
Epoch 50/52
9445953/9445953 [==============================] - 29s 3us/step - loss: 212.9748 - val_loss: 221.3136
Epoch 51/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 212.6848 - val_loss: 218.7927
Epoch 52/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 212.5477 - val_loss: 218.7983

 CHILD 3 out of 6 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 4000

         EPOCHS = 52

         CHILD ACCURACY = 226.53392168661603

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/52
2018-07-16 20:12:32.925530: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 20:12:32.933532: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 26s 3us/step - loss: 302.9824 - val_loss: 279.9630
Epoch 2/52
9445953/9445953 [==============================] - 25s 3us/step - loss: 278.2078 - val_loss: 272.4662
Epoch 3/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 273.6734 - val_loss: 269.3359
Epoch 4/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 269.8703 - val_loss: 265.1954
Epoch 5/52
9445953/9445953 [==============================] - 25s 3us/step - loss: 264.9203 - val_loss: 261.6337
Epoch 6/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 258.6430 - val_loss: 253.0479
Epoch 7/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 253.4870 - val_loss: 248.8933
Epoch 8/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 248.4984 - val_loss: 244.6286
Epoch 9/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 243.1241 - val_loss: 240.7902
Epoch 10/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 238.0568 - val_loss: 235.7958
Epoch 11/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 234.3604 - val_loss: 233.9913
Epoch 12/52
9445953/9445953 [==============================] - 25s 3us/step - loss: 231.5146 - val_loss: 229.8214
Epoch 13/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 229.0578 - val_loss: 241.5894
Epoch 14/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 226.9215 - val_loss: 228.5342
Epoch 15/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 225.2675 - val_loss: 221.0561
Epoch 16/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 223.5218 - val_loss: 220.3854
Epoch 17/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 221.9739 - val_loss: 220.0496
Epoch 18/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 220.9205 - val_loss: 217.2052
Epoch 19/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 219.7883 - val_loss: 222.8824
Epoch 20/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 218.9637 - val_loss: 222.9271
Epoch 21/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 218.0217 - val_loss: 217.7423
Epoch 22/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 217.1827 - val_loss: 220.9888
Epoch 23/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 216.4082 - val_loss: 220.7279
Epoch 24/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 215.6857 - val_loss: 217.3398
Epoch 25/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 215.1034 - val_loss: 215.4320
Epoch 26/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 214.2724 - val_loss: 213.3573
Epoch 27/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 213.8787 - val_loss: 212.0997
Epoch 28/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 213.3803 - val_loss: 216.7621
Epoch 29/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 212.7656 - val_loss: 222.2726
Epoch 30/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 212.5330 - val_loss: 211.4293
Epoch 31/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 211.9095 - val_loss: 211.4393
Epoch 32/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 211.6321 - val_loss: 209.7128
Epoch 33/52
9445953/9445953 [==============================] - 25s 3us/step - loss: 210.9190 - val_loss: 210.1937
Epoch 34/52
9445953/9445953 [==============================] - 25s 3us/step - loss: 210.4476 - val_loss: 225.5684
Epoch 35/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 210.3322 - val_loss: 208.6849
Epoch 36/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 209.7665 - val_loss: 219.7041
Epoch 37/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 209.5237 - val_loss: 214.1075
Epoch 38/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 209.0208 - val_loss: 210.3625
Epoch 39/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 208.6861 - val_loss: 208.1698
Epoch 40/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 208.4367 - val_loss: 208.0055
Epoch 41/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 208.0390 - val_loss: 208.5118
Epoch 42/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 207.8449 - val_loss: 210.2347
Epoch 43/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 207.5059 - val_loss: 209.8207
Epoch 44/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 207.1770 - val_loss: 207.0987
Epoch 45/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 207.0360 - val_loss: 206.4016
Epoch 46/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 206.7374 - val_loss: 210.0652
Epoch 47/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 206.2984 - val_loss: 223.5100
Epoch 48/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 206.1378 - val_loss: 215.4560
Epoch 49/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 205.8603 - val_loss: 209.7976
Epoch 50/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 205.6405 - val_loss: 208.9404
Epoch 51/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 205.2991 - val_loss: 206.4905
Epoch 52/52
9445953/9445953 [==============================] - 25s 3us/step - loss: 205.2002 - val_loss: 207.7809

 CHILD 4 out of 6 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 5600

         EPOCHS = 52

         CHILD ACCURACY = 210.0946448205595


                 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> APPENDED CHILD TO THE QUEUE | QUEUE SIZE = 2 <<<<<<<<<<<<<<<<<<<<<<<<<<<


                         ********** BEST MODEL SAVED | EPOCHS = 52 | BATCH_SIZE = 5600 | ACCURACY = 210.0946448205595 ********



Train on 9445953 samples, validate on 1049551 samples
Epoch 1/52
2018-07-16 20:33:16.760596: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 20:33:16.766907: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 27s 3us/step - loss: 302.4558 - val_loss: 280.7046
Epoch 2/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 279.2657 - val_loss: 283.6967
Epoch 3/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 274.4552 - val_loss: 271.0566
Epoch 4/52
9445953/9445953 [==============================] - 29s 3us/step - loss: 272.0725 - val_loss: 269.6190
Epoch 5/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 269.5119 - val_loss: 264.1842
Epoch 6/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 260.2129 - val_loss: 253.7550
Epoch 7/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 252.4457 - val_loss: 247.3982
Epoch 8/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 247.3357 - val_loss: 242.5202
Epoch 9/52
9445953/9445953 [==============================] - 25s 3us/step - loss: 243.5244 - val_loss: 239.0515
Epoch 10/52
9445953/9445953 [==============================] - 25s 3us/step - loss: 240.3933 - val_loss: 242.0445
Epoch 11/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 238.0007 - val_loss: 233.5550
Epoch 12/52
9445953/9445953 [==============================] - 25s 3us/step - loss: 236.1325 - val_loss: 233.5837
Epoch 13/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 234.5859 - val_loss: 231.8171
Epoch 14/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 233.3045 - val_loss: 237.1175
Epoch 15/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 232.2348 - val_loss: 229.0038
Epoch 16/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 231.0219 - val_loss: 237.3728
Epoch 17/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 230.2618 - val_loss: 230.2697
Epoch 18/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 229.2744 - val_loss: 228.5316
Epoch 19/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 228.4094 - val_loss: 228.4684
Epoch 20/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 227.8254 - val_loss: 242.0414
Epoch 21/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 227.0462 - val_loss: 229.8989
Epoch 22/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 226.5038 - val_loss: 226.2299
Epoch 23/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 225.8259 - val_loss: 222.8241
Epoch 24/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 225.3302 - val_loss: 225.3683
Epoch 25/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 224.7048 - val_loss: 224.4494
Epoch 26/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 224.3253 - val_loss: 222.8043
Epoch 27/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 223.9366 - val_loss: 228.7933
Epoch 28/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 223.1704 - val_loss: 220.1891
Epoch 29/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 222.7834 - val_loss: 233.6827
Epoch 30/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 222.2670 - val_loss: 220.2749
Epoch 31/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 221.9954 - val_loss: 220.6530
Epoch 32/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 221.6250 - val_loss: 223.4698
Epoch 33/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 221.0409 - val_loss: 225.7473
Epoch 34/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 220.8029 - val_loss: 222.3566
Epoch 35/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 220.3132 - val_loss: 222.4700
Epoch 36/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 220.0417 - val_loss: 218.9682
Epoch 37/52
9445953/9445953 [==============================] - 29s 3us/step - loss: 219.6319 - val_loss: 217.7782
Epoch 38/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 219.4873 - val_loss: 225.0165
Epoch 39/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 218.9041 - val_loss: 216.9208
Epoch 40/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 218.7621 - val_loss: 217.7562
Epoch 41/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 218.4677 - val_loss: 215.3178
Epoch 42/52
9445953/9445953 [==============================] - 28s 3us/step - loss: 217.9720 - val_loss: 215.8245
Epoch 43/52
9445953/9445953 [==============================] - 27s 3us/step - loss: 217.8732 - val_loss: 214.8504
Epoch 44/52
9445953/9445953 [==============================] - 26s 3us/step - loss: 217.6883 - val_loss: 216.5037
Epoch 45/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 217.3276 - val_loss: 215.7063
Epoch 46/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 216.8057 - val_loss: 224.9478
Epoch 47/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 216.9511 - val_loss: 218.0978
Epoch 48/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 216.4017 - val_loss: 215.6944
Epoch 49/52
9445953/9445953 [==============================] - 24s 3us/step - loss: 216.1558 - val_loss: 219.5431
Epoch 50/52
9445953/9445953 [==============================] - 24s 2us/step - loss: 215.9395 - val_loss: 220.3172
Epoch 51/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.6103 - val_loss: 219.3670
Epoch 52/52
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.3441 - val_loss: 230.1272

 CHILD 5 out of 6 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 4800

         EPOCHS = 52

         CHILD ACCURACY = 230.90956599869386

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/46
2018-07-16 20:56:19.998616: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 20:56:20.003953: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 26s 3us/step - loss: 300.2174 - val_loss: 277.7098
Epoch 2/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 277.8748 - val_loss: 275.7975
Epoch 3/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 273.5132 - val_loss: 276.0016
Epoch 4/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 262.9892 - val_loss: 258.3245
Epoch 5/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 254.2252 - val_loss: 249.9256
Epoch 6/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 247.7797 - val_loss: 243.1309
Epoch 7/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 242.7552 - val_loss: 240.6130
Epoch 8/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 239.0115 - val_loss: 234.9548
Epoch 9/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 236.3754 - val_loss: 232.8602
Epoch 10/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 234.0037 - val_loss: 231.1790
Epoch 11/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 232.3384 - val_loss: 233.8297
Epoch 12/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 230.6466 - val_loss: 231.7183
Epoch 13/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 229.3338 - val_loss: 229.0353
Epoch 14/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 228.3188 - val_loss: 230.5184
Epoch 15/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 227.1504 - val_loss: 223.2239
Epoch 16/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 226.0990 - val_loss: 222.0102
Epoch 17/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 225.1962 - val_loss: 221.1962
Epoch 18/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 224.4576 - val_loss: 222.7465
Epoch 19/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 223.6818 - val_loss: 222.5280
Epoch 20/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 222.9060 - val_loss: 222.7118
Epoch 21/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 222.3079 - val_loss: 223.9444
Epoch 22/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 221.8679 - val_loss: 220.9949
Epoch 23/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 221.2240 - val_loss: 232.3967
Epoch 24/46
9445953/9445953 [==============================] - 212s 22us/step - loss: 220.5508 - val_loss: 227.2526
Epoch 25/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 220.0334 - val_loss: 223.4751
Epoch 26/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 219.5912 - val_loss: 222.6285
Epoch 27/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 219.1874 - val_loss: 215.7064
Epoch 28/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 218.6628 - val_loss: 220.0995
Epoch 29/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 218.1188 - val_loss: 218.3090
Epoch 30/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 217.7699 - val_loss: 224.0404
Epoch 31/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 217.2348 - val_loss: 216.1936
Epoch 32/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 216.8728 - val_loss: 230.6053
Epoch 33/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 216.5470 - val_loss: 217.7562
Epoch 34/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 216.2786 - val_loss: 221.2121
Epoch 35/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 215.6972 - val_loss: 215.2494
Epoch 36/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 215.4418 - val_loss: 213.8094
Epoch 37/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 215.0653 - val_loss: 216.8858
Epoch 38/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 214.7961 - val_loss: 218.0109
Epoch 39/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 214.3184 - val_loss: 227.0237
Epoch 40/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 214.1773 - val_loss: 218.9835
Epoch 41/46
9445953/9445953 [==============================] - 25s 3us/step - loss: 213.6866 - val_loss: 226.6906
Epoch 42/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 213.4130 - val_loss: 219.4072
Epoch 43/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 213.0999 - val_loss: 211.4248
Epoch 44/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 212.9366 - val_loss: 210.4239
Epoch 45/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 212.6503 - val_loss: 225.2912
Epoch 46/46
9445953/9445953 [==============================] - 26s 3us/step - loss: 212.4153 - val_loss: 215.4322

 CHILD 6 out of 6 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 4000

         EPOCHS = 46

         CHILD ACCURACY = 221.4307927917139



 QUEUE SIZE BEFORE GET = 2


 NODE SELECTED <<<------ Epochs = 52 -------- BatchSize = 5600--------- Accuracy = 210.0946448205595 -------->>>

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/49
2018-07-16 21:18:48.150266: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 21:18:48.156682: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 25s 3us/step - loss: 302.1135 - val_loss: 277.8132
Epoch 2/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 277.9575 - val_loss: 274.1037
Epoch 3/49
9445953/9445953 [==============================] - 25s 3us/step - loss: 273.9361 - val_loss: 270.7986
Epoch 4/49
9445953/9445953 [==============================] - 24s 3us/step - loss: 272.1585 - val_loss: 269.3630
Epoch 5/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 270.5754 - val_loss: 267.3349
Epoch 6/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 265.7720 - val_loss: 257.4089
Epoch 7/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 256.2159 - val_loss: 250.3774
Epoch 8/49
9445953/9445953 [==============================] - 24s 2us/step - loss: 249.8998 - val_loss: 245.2644
Epoch 9/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 245.2201 - val_loss: 242.8019
Epoch 10/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 241.2397 - val_loss: 252.3055
Epoch 11/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 238.2803 - val_loss: 235.1875
Epoch 12/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 235.8533 - val_loss: 233.1209
Epoch 13/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 233.9754 - val_loss: 232.3242
Epoch 14/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 232.1680 - val_loss: 232.4996
Epoch 15/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 230.6502 - val_loss: 229.6276
Epoch 16/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 229.2705 - val_loss: 227.8585
Epoch 17/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 227.9862 - val_loss: 231.9791
Epoch 18/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 227.0466 - val_loss: 232.9546
Epoch 19/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 226.2239 - val_loss: 225.0190
Epoch 20/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 225.1991 - val_loss: 226.3097
Epoch 21/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 224.3270 - val_loss: 225.2552
Epoch 22/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 223.7016 - val_loss: 229.5962
Epoch 23/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 223.1720 - val_loss: 219.7159
Epoch 24/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 222.3075 - val_loss: 222.5160
Epoch 25/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 221.8084 - val_loss: 220.7699
Epoch 26/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 221.2605 - val_loss: 219.2916
Epoch 27/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 220.6960 - val_loss: 222.0240
Epoch 28/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 220.2602 - val_loss: 232.7234
Epoch 29/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 219.8599 - val_loss: 219.9511
Epoch 30/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 219.2158 - val_loss: 217.9051
Epoch 31/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 218.8076 - val_loss: 218.0738
Epoch 32/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 218.3065 - val_loss: 214.5684
Epoch 33/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 218.0113 - val_loss: 214.6773
Epoch 34/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 217.6596 - val_loss: 216.4882
Epoch 35/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 217.2907 - val_loss: 220.5963
Epoch 36/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 216.9829 - val_loss: 225.8123
Epoch 37/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 216.4192 - val_loss: 216.9466
Epoch 38/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 216.2136 - val_loss: 216.4505
Epoch 39/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.6443 - val_loss: 214.9739
Epoch 40/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.5629 - val_loss: 217.2858
Epoch 41/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.3000 - val_loss: 213.4744
Epoch 42/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 215.0616 - val_loss: 220.7605
Epoch 43/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 214.6243 - val_loss: 221.6151
Epoch 44/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 214.4657 - val_loss: 222.3808
Epoch 45/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 214.1179 - val_loss: 218.4067
Epoch 46/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 213.9200 - val_loss: 212.9241
Epoch 47/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 213.5218 - val_loss: 218.9685
Epoch 48/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 213.2809 - val_loss: 214.9104
Epoch 49/49
9445953/9445953 [==============================] - 23s 2us/step - loss: 213.1854 - val_loss: 213.9349

 CHILD 1 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 4800

         EPOCHS = 49

         CHILD ACCURACY = 218.54647846848113

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/52
2018-07-16 21:37:36.277826: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 21:37:36.283241: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 20s 2us/step - loss: 367.6031 - val_loss: 347.3194
Epoch 2/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 338.7083 - val_loss: 327.5121
Epoch 3/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 324.9251 - val_loss: 319.1509
Epoch 4/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 320.1184 - val_loss: 317.1990
Epoch 5/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3317 - val_loss: 317.0894
Epoch 6/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0905
Epoch 7/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0906
Epoch 8/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 9/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 10/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0896
Epoch 11/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3030 - val_loss: 317.0916
Epoch 12/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0912
Epoch 13/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 14/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0893
Epoch 15/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0895
Epoch 16/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0906
Epoch 17/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0905
Epoch 18/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0903
Epoch 19/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0905
Epoch 20/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0896
Epoch 21/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 22/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3030 - val_loss: 317.0893
Epoch 23/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3030 - val_loss: 317.0907
Epoch 24/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0898
Epoch 25/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0905
Epoch 26/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0898
Epoch 27/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0913
Epoch 28/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3029 - val_loss: 317.0894
Epoch 29/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 30/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0893
Epoch 31/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0911
Epoch 32/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0923
Epoch 33/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0907
Epoch 34/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0904
Epoch 35/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 36/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0915
Epoch 37/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0912
Epoch 38/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0896
Epoch 39/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0906
Epoch 40/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0895
Epoch 41/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 42/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0908
Epoch 43/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 44/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0907
Epoch 45/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3030 - val_loss: 317.0894
Epoch 46/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0894
Epoch 47/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3033 - val_loss: 317.0900
Epoch 48/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0894
Epoch 49/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0908
Epoch 50/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0903
Epoch 51/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3031 - val_loss: 317.0897
Epoch 52/52
9445953/9445953 [==============================] - 20s 2us/step - loss: 319.3032 - val_loss: 317.0920

 CHILD 2 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 6400

         EPOCHS = 52

         CHILD ACCURACY = 314.87822569288466

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/55
2018-07-16 21:54:39.311581: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 21:54:39.317231: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 23s 2us/step - loss: 305.2379 - val_loss: 280.3680
Epoch 2/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 281.1704 - val_loss: 278.0225
Epoch 3/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 276.7893 - val_loss: 273.6548
Epoch 4/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 273.4141 - val_loss: 270.8160
Epoch 5/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 272.1177 - val_loss: 269.4666
Epoch 6/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 271.4415 - val_loss: 268.7921
Epoch 7/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 270.3500 - val_loss: 267.7368
Epoch 8/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 268.4939 - val_loss: 265.9145
Epoch 9/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 264.7264 - val_loss: 258.6634
Epoch 10/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 257.6626 - val_loss: 250.5819
Epoch 11/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 251.7979 - val_loss: 250.2445
Epoch 12/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 247.6302 - val_loss: 242.3641
Epoch 13/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 244.2446 - val_loss: 238.4845
Epoch 14/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 241.5595 - val_loss: 236.0584
Epoch 15/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 239.2936 - val_loss: 235.0511
Epoch 16/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 237.4495 - val_loss: 232.4076
Epoch 17/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 235.8712 - val_loss: 252.6547
Epoch 18/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 234.5635 - val_loss: 230.1305
Epoch 19/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 233.3700 - val_loss: 238.6099
Epoch 20/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 232.2857 - val_loss: 233.9656
Epoch 21/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 231.2852 - val_loss: 228.4788
Epoch 22/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 230.4101 - val_loss: 230.9660
Epoch 23/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 229.5976 - val_loss: 227.8243
Epoch 24/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 228.8862 - val_loss: 232.6069
Epoch 25/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 228.1957 - val_loss: 225.6441
Epoch 26/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 227.4755 - val_loss: 224.2508
Epoch 27/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 226.8313 - val_loss: 226.0091
Epoch 28/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 226.3311 - val_loss: 224.5586
Epoch 29/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 225.6534 - val_loss: 224.2178
Epoch 30/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 225.3395 - val_loss: 224.6514
Epoch 31/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 224.7352 - val_loss: 221.7754
Epoch 32/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 224.3482 - val_loss: 224.0785
Epoch 33/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 223.8370 - val_loss: 222.0642
Epoch 34/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 223.2761 - val_loss: 233.8787
Epoch 35/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 222.8833 - val_loss: 223.5951
Epoch 36/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 222.6575 - val_loss: 221.9289
Epoch 37/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 222.1419 - val_loss: 219.7730
Epoch 38/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 221.9304 - val_loss: 218.9966
Epoch 39/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 221.5575 - val_loss: 220.1280
Epoch 40/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 221.2705 - val_loss: 220.0278
Epoch 41/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 220.8187 - val_loss: 220.5582
Epoch 42/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 220.4445 - val_loss: 219.5984
Epoch 43/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 220.1703 - val_loss: 219.0645
Epoch 44/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 219.8693 - val_loss: 221.2291
Epoch 45/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 219.7928 - val_loss: 221.9265
Epoch 46/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 219.3240 - val_loss: 221.1998
Epoch 47/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 218.9214 - val_loss: 222.0465
Epoch 48/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 218.7430 - val_loss: 222.1846
Epoch 49/55
9445953/9445953 [==============================] - 24s 3us/step - loss: 218.5498 - val_loss: 218.5414
Epoch 50/55
9445953/9445953 [==============================] - 27s 3us/step - loss: 218.2835 - val_loss: 220.1040
Epoch 51/55
9445953/9445953 [==============================] - 25s 3us/step - loss: 217.8777 - val_loss: 216.0676
Epoch 52/55
9445953/9445953 [==============================] - 25s 3us/step - loss: 217.7942 - val_loss: 217.1467
Epoch 53/55
9445953/9445953 [==============================] - 24s 3us/step - loss: 217.5223 - val_loss: 214.5428
Epoch 54/55
9445953/9445953 [==============================] - 25s 3us/step - loss: 217.3105 - val_loss: 215.5837
Epoch 55/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 217.1318 - val_loss: 216.3422

 CHILD 3 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 4800

         EPOCHS = 55

         CHILD ACCURACY = 213.6575950824686

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/55
2018-07-16 22:15:53.088856: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 22:15:53.094291: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 24s 2us/step - loss: 305.8003 - val_loss: 283.9424
Epoch 2/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 280.9602 - val_loss: 281.8730
Epoch 3/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 276.5592 - val_loss: 271.9557
Epoch 4/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 272.2047 - val_loss: 269.3258
Epoch 5/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 265.1106 - val_loss: 259.4398
Epoch 6/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 257.8194 - val_loss: 251.1344
Epoch 7/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 251.5879 - val_loss: 245.0183
Epoch 8/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 246.7557 - val_loss: 240.1771
Epoch 9/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 242.5125 - val_loss: 245.1197
Epoch 10/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 239.0455 - val_loss: 233.2342
Epoch 11/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 235.6944 - val_loss: 231.9469
Epoch 12/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 232.9941 - val_loss: 227.0086
Epoch 13/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 230.5301 - val_loss: 229.0909
Epoch 14/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 228.5421 - val_loss: 243.5511
Epoch 15/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 226.7943 - val_loss: 223.7979
Epoch 16/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 225.0459 - val_loss: 229.0123
Epoch 17/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 223.6279 - val_loss: 219.9890
Epoch 18/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 222.5149 - val_loss: 223.5154
Epoch 19/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 221.3026 - val_loss: 232.8464
Epoch 20/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 220.1640 - val_loss: 222.1496
Epoch 21/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 219.3740 - val_loss: 218.8553
Epoch 22/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 218.3580 - val_loss: 218.9578
Epoch 23/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 217.5376 - val_loss: 216.9430
Epoch 24/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 216.7441 - val_loss: 215.4667
Epoch 25/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 216.0693 - val_loss: 228.2171
Epoch 26/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 215.4769 - val_loss: 211.3229
Epoch 27/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 214.7586 - val_loss: 223.0134
Epoch 28/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 214.0315 - val_loss: 211.9043
Epoch 29/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 213.4307 - val_loss: 227.8461
Epoch 30/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 213.0621 - val_loss: 212.4771
Epoch 31/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 212.3188 - val_loss: 218.1247
Epoch 32/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 211.9288 - val_loss: 211.0901
Epoch 33/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 211.2756 - val_loss: 212.0773
Epoch 34/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 210.8787 - val_loss: 219.5376
Epoch 35/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 210.4554 - val_loss: 213.1136
Epoch 36/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 209.8977 - val_loss: 208.2222
Epoch 37/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 209.4496 - val_loss: 213.3073
Epoch 38/55
9445953/9445953 [==============================] - 23s 2us/step - loss: 209.2247 - val_loss: 214.6542
Epoch 39/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 208.7303 - val_loss: 208.4309
Epoch 40/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 208.4026 - val_loss: 208.4978
Epoch 41/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 207.8424 - val_loss: 209.8402
Epoch 42/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 207.5907 - val_loss: 214.5131
Epoch 43/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 207.3111 - val_loss: 207.6010
Epoch 44/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 207.0399 - val_loss: 208.7668
Epoch 45/55
9445953/9445953 [==============================] - 20s 2us/step - loss: 206.6092 - val_loss: 210.9275
Epoch 46/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 206.1806 - val_loss: 212.9551
Epoch 47/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 205.7655 - val_loss: 209.1561
Epoch 48/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 205.7528 - val_loss: 210.2525
Epoch 49/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 205.1893 - val_loss: 204.1665
Epoch 50/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 204.9845 - val_loss: 205.0998
Epoch 51/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 204.5493 - val_loss: 213.7107
Epoch 52/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 204.4504 - val_loss: 210.5473
Epoch 53/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 204.2775 - val_loss: 219.7712
Epoch 54/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 203.7150 - val_loss: 204.9692
Epoch 55/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 203.6188 - val_loss: 202.7399

 CHILD 4 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 6400

         EPOCHS = 55

         CHILD ACCURACY = 214.44981895745929

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/55
2018-07-16 22:36:02.814071: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 22:36:02.819703: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 23s 2us/step - loss: 302.5044 - val_loss: 281.5473
Epoch 2/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 281.0527 - val_loss: 276.0921
Epoch 3/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 277.4912 - val_loss: 274.7632
Epoch 4/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 276.0076 - val_loss: 273.3291
Epoch 5/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 275.0567 - val_loss: 272.5162
Epoch 6/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 273.7781 - val_loss: 270.1021
Epoch 7/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 269.5973 - val_loss: 266.0739
Epoch 8/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 264.5139 - val_loss: 259.9346
Epoch 9/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 260.6854 - val_loss: 256.4260
Epoch 10/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 257.9647 - val_loss: 260.2830
Epoch 11/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 255.7319 - val_loss: 255.5598
Epoch 12/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 254.0272 - val_loss: 251.8812
Epoch 13/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 252.6454 - val_loss: 250.3567
Epoch 14/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 251.4329 - val_loss: 250.5065
Epoch 15/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 250.5173 - val_loss: 251.8012
Epoch 16/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 249.5949 - val_loss: 248.7703
Epoch 17/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 248.8950 - val_loss: 247.8384
Epoch 18/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 248.1472 - val_loss: 246.6405
Epoch 19/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 247.5960 - val_loss: 246.8980
Epoch 20/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 246.9281 - val_loss: 245.5230
Epoch 21/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 246.3929 - val_loss: 245.1967
Epoch 22/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 245.8186 - val_loss: 246.0496
Epoch 23/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 245.2711 - val_loss: 245.2574
Epoch 24/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 244.7787 - val_loss: 246.8919
Epoch 25/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 244.2581 - val_loss: 243.5609
Epoch 26/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 243.6409 - val_loss: 241.6285
Epoch 27/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 243.1337 - val_loss: 242.9906
Epoch 28/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 242.5529 - val_loss: 241.3317
Epoch 29/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 241.9966 - val_loss: 241.7592
Epoch 30/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 241.4326 - val_loss: 242.1830
Epoch 31/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 240.9359 - val_loss: 242.4897
Epoch 32/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 240.4241 - val_loss: 240.3732
Epoch 33/55
9445953/9445953 [==============================] - 22s 2us/step - loss: 240.0042 - val_loss: 239.1656
Epoch 34/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 239.6139 - val_loss: 240.9730
Epoch 35/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 239.1255 - val_loss: 238.5622
Epoch 36/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 238.6421 - val_loss: 238.0292
Epoch 37/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 238.2874 - val_loss: 238.4954
Epoch 38/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 237.8163 - val_loss: 236.4018
Epoch 39/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 237.3032 - val_loss: 239.7261
Epoch 40/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 236.9043 - val_loss: 236.7863
Epoch 41/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 236.2556 - val_loss: 236.3965
Epoch 42/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 234.7856 - val_loss: 230.8682
Epoch 43/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 230.0958 - val_loss: 227.9687
Epoch 44/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 227.1696 - val_loss: 226.7895
Epoch 45/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 225.1065 - val_loss: 224.8113
Epoch 46/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 223.5751 - val_loss: 224.5748
Epoch 47/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 222.5658 - val_loss: 222.7255
Epoch 48/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 221.4272 - val_loss: 221.3607
Epoch 49/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 220.6139 - val_loss: 222.6952
Epoch 50/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 219.9444 - val_loss: 228.8354
Epoch 51/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 219.2334 - val_loss: 220.0130
Epoch 52/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 218.6031 - val_loss: 224.1549
Epoch 53/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 218.1136 - val_loss: 216.6908
Epoch 54/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 217.4967 - val_loss: 219.5710
Epoch 55/55
9445953/9445953 [==============================] - 21s 2us/step - loss: 216.9795 - val_loss: 224.7961

 CHILD 5 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 5600

         EPOCHS = 55

         CHILD ACCURACY = 223.1753791386902



 QUEUE SIZE BEFORE GET = 1


 NODE SELECTED <<<------ Epochs = 43 -------- BatchSize = 7200--------- Accuracy = 219.81897184339084 -------->>>

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/40
2018-07-16 22:55:50.506829: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 22:55:50.511701: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 19s 2us/step - loss: 369.3438 - val_loss: 352.3001
Epoch 2/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 344.4125 - val_loss: 333.2129
Epoch 3/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 329.7788 - val_loss: 322.7690
Epoch 4/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 322.4587 - val_loss: 318.3108
Epoch 5/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.8309 - val_loss: 317.1827
Epoch 6/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3326 - val_loss: 317.0895
Epoch 7/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3034 - val_loss: 317.0899
Epoch 8/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3031 - val_loss: 317.0898
Epoch 9/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 10/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3031 - val_loss: 317.0910
Epoch 11/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3030 - val_loss: 317.0903
Epoch 12/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3032 - val_loss: 317.0907
Epoch 13/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3032 - val_loss: 317.0897
Epoch 14/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 15/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3031 - val_loss: 317.0895
Epoch 16/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3031 - val_loss: 317.0905
Epoch 17/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 18/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3030 - val_loss: 317.0910
Epoch 19/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3031 - val_loss: 317.0898
Epoch 20/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3031 - val_loss: 317.0916
Epoch 21/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3033 - val_loss: 317.0899
Epoch 22/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3031 - val_loss: 317.0906
Epoch 23/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3032 - val_loss: 317.0902
Epoch 24/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3031 - val_loss: 317.0900
Epoch 25/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 26/40
9445953/9445953 [==============================] - 18s 2us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 27/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0909
Epoch 28/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0904
Epoch 29/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0904
Epoch 30/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 31/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0896
Epoch 32/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0896
Epoch 33/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3030 - val_loss: 317.0928
Epoch 34/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 35/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0896
Epoch 36/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0894
Epoch 37/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0902
Epoch 38/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0894
Epoch 39/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0896
Epoch 40/40
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0894

 CHILD 1 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 8000

         EPOCHS = 40

         CHILD ACCURACY = 314.87581127892764

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/43
2018-07-16 23:08:16.854705: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 23:08:16.861170: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 19s 2us/step - loss: 369.5932 - val_loss: 352.4196
Epoch 2/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 344.5149 - val_loss: 333.2922
Epoch 3/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 329.8414 - val_loss: 322.8148
Epoch 4/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 322.4928 - val_loss: 318.3303
Epoch 5/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.8440 - val_loss: 317.1889
Epoch 6/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3354 - val_loss: 317.0895
Epoch 7/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3034 - val_loss: 317.0900
Epoch 8/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3030 - val_loss: 317.0904
Epoch 9/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3030 - val_loss: 317.0893
Epoch 10/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0905
Epoch 11/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0901
Epoch 12/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 13/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3030 - val_loss: 317.0902
Epoch 14/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0893
Epoch 15/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0910
Epoch 16/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0908
Epoch 17/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0903
Epoch 18/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0912
Epoch 19/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 20/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0894
Epoch 21/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0906
Epoch 22/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0899
Epoch 23/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0911
Epoch 24/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0902
Epoch 25/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0910
Epoch 26/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0904
Epoch 27/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0900
Epoch 28/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0907
Epoch 29/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3033 - val_loss: 317.0901
Epoch 30/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 31/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0906
Epoch 32/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0903
Epoch 33/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0903
Epoch 34/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0905
Epoch 35/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0898
Epoch 36/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 37/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3033 - val_loss: 317.0908
Epoch 38/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0896
Epoch 39/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3030 - val_loss: 317.0908
Epoch 40/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3033 - val_loss: 317.0903
Epoch 41/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3031 - val_loss: 317.0893
Epoch 42/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3033 - val_loss: 317.0896
Epoch 43/43
9445953/9445953 [==============================] - 19s 2us/step - loss: 319.3032 - val_loss: 317.0908

 CHILD 2 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 8000

         EPOCHS = 43

         CHILD ACCURACY = 314.8766586568774

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/46
2018-07-16 23:21:58.236978: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 23:21:58.242845: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 21s 2us/step - loss: 302.7504 - val_loss: 284.0178
Epoch 2/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 281.9670 - val_loss: 276.2301
Epoch 3/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 276.8321 - val_loss: 271.5878
Epoch 4/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 273.0236 - val_loss: 268.3459
Epoch 5/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 269.0759 - val_loss: 264.9874
Epoch 6/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 265.4679 - val_loss: 263.1088
Epoch 7/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 262.5504 - val_loss: 260.5464
Epoch 8/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 260.0537 - val_loss: 256.6433
Epoch 9/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 257.9497 - val_loss: 256.0694
Epoch 10/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 256.1167 - val_loss: 253.7956
Epoch 11/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 254.3130 - val_loss: 250.8161
Epoch 12/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 252.6909 - val_loss: 250.5512
Epoch 13/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 251.2202 - val_loss: 247.5705
Epoch 14/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 249.8267 - val_loss: 246.6808
Epoch 15/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 248.6478 - val_loss: 247.1454
Epoch 16/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 247.5644 - val_loss: 247.1947
Epoch 17/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 246.6387 - val_loss: 246.5907
Epoch 18/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 245.7880 - val_loss: 245.1901
Epoch 19/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 244.9897 - val_loss: 242.5051
Epoch 20/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 244.1717 - val_loss: 244.7152
Epoch 21/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 243.2576 - val_loss: 240.8048
Epoch 22/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 242.3429 - val_loss: 239.9730
Epoch 23/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 238.3082 - val_loss: 231.2009
Epoch 24/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 232.9499 - val_loss: 227.2212
Epoch 25/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 229.7837 - val_loss: 226.0780
Epoch 26/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 227.7297 - val_loss: 223.5491
Epoch 27/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 225.9921 - val_loss: 226.4715
Epoch 28/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 224.8346 - val_loss: 221.9707
Epoch 29/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 223.5939 - val_loss: 224.6614
Epoch 30/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 222.6597 - val_loss: 219.8289
Epoch 31/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 221.7371 - val_loss: 220.6112
Epoch 32/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 221.0374 - val_loss: 220.6574
Epoch 33/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 220.3525 - val_loss: 223.4372
Epoch 34/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 219.7135 - val_loss: 222.2428
Epoch 35/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 219.1457 - val_loss: 217.9314
Epoch 36/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 218.5766 - val_loss: 219.0295
Epoch 37/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 218.1818 - val_loss: 215.7336
Epoch 38/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 217.6215 - val_loss: 215.1917
Epoch 39/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 217.1224 - val_loss: 214.1221
Epoch 40/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 216.8497 - val_loss: 216.5684
Epoch 41/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 216.4314 - val_loss: 213.0351
Epoch 42/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 215.8567 - val_loss: 217.6442
Epoch 43/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 215.5592 - val_loss: 220.2555
Epoch 44/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 215.1539 - val_loss: 217.4095
Epoch 45/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 214.8780 - val_loss: 220.7244
Epoch 46/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 214.5320 - val_loss: 220.9913

 CHILD 3 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 7200

         EPOCHS = 46

         CHILD ACCURACY = 224.56222632992237

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/46
2018-07-16 23:37:12.450213: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 23:37:12.455486: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 19s 2us/step - loss: 312.5922 - val_loss: 285.2061
Epoch 2/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 283.3303 - val_loss: 278.4050
Epoch 3/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 276.5208 - val_loss: 271.8748
Epoch 4/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 273.6478 - val_loss: 271.4152
Epoch 5/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 271.9896 - val_loss: 271.3964
Epoch 6/46
9445953/9445953 [==============================] - 22s 2us/step - loss: 270.7890 - val_loss: 268.1803
Epoch 7/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 269.8876 - val_loss: 268.1820
Epoch 8/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 268.8831 - val_loss: 266.4504
Epoch 9/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 267.8394 - val_loss: 265.2914
Epoch 10/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 265.2353 - val_loss: 258.4004
Epoch 11/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 256.5117 - val_loss: 247.7981
Epoch 12/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 249.7960 - val_loss: 248.6693
Epoch 13/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 244.9516 - val_loss: 242.2571
Epoch 14/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 241.2731 - val_loss: 235.5665
Epoch 15/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 238.3302 - val_loss: 236.1023
Epoch 16/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 235.8387 - val_loss: 231.2014
Epoch 17/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 234.0521 - val_loss: 232.0076
Epoch 18/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 232.1588 - val_loss: 230.6838
Epoch 19/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 230.8017 - val_loss: 226.6397
Epoch 20/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 229.3256 - val_loss: 235.6753
Epoch 21/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 228.1520 - val_loss: 228.3812
Epoch 22/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 227.2016 - val_loss: 224.0520
Epoch 23/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 226.1862 - val_loss: 226.7781
Epoch 24/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 225.3667 - val_loss: 222.5536
Epoch 25/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 224.5564 - val_loss: 220.0777
Epoch 26/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 223.8802 - val_loss: 220.5026
Epoch 27/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 223.1189 - val_loss: 220.8700
Epoch 28/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 222.7843 - val_loss: 223.5497
Epoch 29/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 221.9337 - val_loss: 237.1442
Epoch 30/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 221.5104 - val_loss: 234.9742
Epoch 31/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 220.8727 - val_loss: 223.8644
Epoch 32/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 220.2818 - val_loss: 218.1347
Epoch 33/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 219.8337 - val_loss: 215.8120
Epoch 34/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 219.2908 - val_loss: 224.8971
Epoch 35/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 218.8272 - val_loss: 220.4925
Epoch 36/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 218.3943 - val_loss: 219.5754
Epoch 37/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 217.9786 - val_loss: 225.2218
Epoch 38/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 217.5237 - val_loss: 223.0123
Epoch 39/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 217.0613 - val_loss: 214.5869
Epoch 40/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 216.7398 - val_loss: 215.1220
Epoch 41/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 216.4968 - val_loss: 224.9371
Epoch 42/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 216.0452 - val_loss: 213.8229
Epoch 43/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 215.7292 - val_loss: 221.7271
Epoch 44/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 215.4965 - val_loss: 213.7205
Epoch 45/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 214.9236 - val_loss: 217.5134
Epoch 46/46
9445953/9445953 [==============================] - 19s 2us/step - loss: 214.7158 - val_loss: 229.9864

 CHILD 4 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 8000

         EPOCHS = 46

         CHILD ACCURACY = 225.76834034894125

Train on 9445953 samples, validate on 1049551 samples
Epoch 1/46
2018-07-16 23:52:01.107300: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-07-16 23:52:01.113875: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 40 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
9445953/9445953 [==============================] - 21s 2us/step - loss: 302.9275 - val_loss: 283.9528
Epoch 2/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 283.7525 - val_loss: 278.4462
Epoch 3/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 279.3953 - val_loss: 275.7369
Epoch 4/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 277.7070 - val_loss: 275.0119
Epoch 5/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 276.7211 - val_loss: 275.9470
Epoch 6/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 276.1191 - val_loss: 274.9244
Epoch 7/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 275.5291 - val_loss: 273.5988
Epoch 8/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 273.7595 - val_loss: 267.5077
Epoch 9/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 268.5515 - val_loss: 264.1911
Epoch 10/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 264.8135 - val_loss: 262.0965
Epoch 11/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 262.2853 - val_loss: 260.2698
Epoch 12/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 260.4522 - val_loss: 256.8801
Epoch 13/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 259.0963 - val_loss: 256.1816
Epoch 14/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 258.1484 - val_loss: 255.4741
Epoch 15/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 257.3133 - val_loss: 255.8912
Epoch 16/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 256.6238 - val_loss: 258.3447
Epoch 17/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 256.0769 - val_loss: 256.1817
Epoch 18/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 255.5033 - val_loss: 253.3675
Epoch 19/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 255.1300 - val_loss: 253.4342
Epoch 20/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 254.7192 - val_loss: 257.5833
Epoch 21/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 254.3949 - val_loss: 253.2402
Epoch 22/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 254.0976 - val_loss: 252.1745
Epoch 23/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 253.7501 - val_loss: 256.7890
Epoch 24/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 253.5019 - val_loss: 254.8833
Epoch 25/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 253.2362 - val_loss: 253.0884
Epoch 26/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 252.9888 - val_loss: 251.5221
Epoch 27/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 252.6838 - val_loss: 251.1653
Epoch 28/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 252.4806 - val_loss: 250.2673
Epoch 29/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 252.2400 - val_loss: 251.0098
Epoch 30/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 252.0394 - val_loss: 250.1831
Epoch 31/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 251.8349 - val_loss: 252.6277
Epoch 32/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 251.6474 - val_loss: 250.4008
Epoch 33/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 251.4480 - val_loss: 252.2170
Epoch 34/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 251.3279 - val_loss: 249.9728
Epoch 35/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 251.1637 - val_loss: 252.2307
Epoch 36/46
9445953/9445953 [==============================] - 21s 2us/step - loss: 250.9407 - val_loss: 256.1910
Epoch 37/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 250.8397 - val_loss: 250.2608
Epoch 38/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 250.5417 - val_loss: 251.8918
Epoch 39/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 250.4466 - val_loss: 250.8336
Epoch 40/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 250.3356 - val_loss: 249.5428
Epoch 41/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 250.1994 - val_loss: 252.5601
Epoch 42/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 250.0824 - val_loss: 255.2713
Epoch 43/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 249.9587 - val_loss: 250.9630
Epoch 44/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 249.7678 - val_loss: 249.6478
Epoch 45/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 249.7252 - val_loss: 250.1306
Epoch 46/46
9445953/9445953 [==============================] - 20s 2us/step - loss: 249.5157 - val_loss: 249.6994

 CHILD 5 out of 5 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

         BATCH SIZE = 6400

         EPOCHS = 46

         CHILD ACCURACY = 249.9823886916826




 #########################  TRAINING COMPLETED  ###############################



 NODES EXPLORED:
(46, 7200)
(40, 5600)
(46, 5600)
(40, 7200)
(43, 7200)
(43, 5600)
(46, 6400)
(40, 6400)
(46, 4800)
(43, 4800)
(49, 6400)
(43, 6400)
(49, 5600)
(52, 5600)
(46, 4000)
(52, 4000)
(49, 4000)
(52, 4800)
(55, 6400)
(49, 4800)
(55, 4800)
(46, 8000)
(40, 8000)
(43, 8000)

C:\Users\mrinal\Documents\code_stuff\Carom Billiards Source>