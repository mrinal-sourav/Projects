
# coding: utf-8

# In[1]:


import re 
from sys import argv, getsizeof
import os
from collections import defaultdict
import json 


# In[2]:


documented_terms = [] #contains the set of "terms", "tokens", "words" along with filenumber and 
                        #the frequency of occurence of the term in each file. 
    
#read the file generated by RunDataTransformer.py 
with open('tokenizer_results.txt') as file:
    for line in file.readlines():
        line = line.strip('\n')
        documented_terms.append(eval(line))


# In[5]:


token_number = 0 
inv_index = {} 
index_mapping = []
term_list = []
document_map = {}
#create an empty doc mapping 


document_map = defaultdict(list)

for term_details in documented_terms: 
    term, doc, freq = term_details
    document_map[doc]+= [freq]
    term_list.append(term)
term_set = set(term_list) 

for term in term_set: 
    index_mapping.append((token_number, term))
    token_number+= 1


# In[6]:


def get_term_index(term):
    for term_map in index_mapping:
        if term_map[1] == term:
            return term_map[0]


# In[7]:


#create an empty dictionary
inv_index = {} 
for count in range(len(term_set)):
    inv_index[count]= []

for term in term_set:
    for term_details in documented_terms:
        other_term, other_doc, other_freq = term_details
        if term == other_term: 
            indx = get_term_index(term)
            inv_index[indx]+= [(other_doc, other_freq)]


# In[8]:


#writing term_mapping onto a file 
with open('InvertedIndex.txt', 'wt', encoding='utf-8') as d: 
    json.dump(inv_index, d)
    

# In[9]:


index_file_list = []
for index in range(len(inv_index)): 
    for key, term in index_mapping:
        if index == key: 
            doc_list = inv_index[index]
            index_file_list.append((index, term, len(doc_list)))
 
    
    


# In[10]:


#writing term_mapping onto a file 
with open('TermIDFile.txt', 'wt', encoding='utf-8') as d: 
    for item in index_file_list:
        d.write("{}\n".format(item))


# In[11]:


doc_file_list = []
for doc_id in document_map: 
    list_of_term_frequency = document_map[doc_id]
    total_terms = sum(list_of_term_frequency) 
    doc_name = 'file_' + str(doc_id)
    doc_file_list+= [(doc_id, doc_name, total_terms)]


# In[12]:


#writing doc_mapping onto a file 
with open('DocumentIDFile.txt', 'wt', encoding='utf-8') as d: 
    for item in doc_file_list:
        d.write("{}\n".format(item))


# In[13]:


doc_id_size = os.path.getsize("DocumentIDFile.txt")


# In[14]:


index_size = os.path.getsize("InvertedIndex.txt")


# In[15]:


term_id_size = os.path.getsize("TermIDFile.txt")


# In[16]:


total_size_of_files = doc_id_size + term_id_size + index_size
ratio = index_size / total_size_of_files
with open('stats.txt', 'at', encoding='utf-8') as d: 
    d.write("\n\t Total size of the three index files (in bytes): {} \n".format(total_size_of_files))
    d.write("\n\t Ratio of the index size to the total file size: {} \n".format(ratio))


# In[ ]:




